{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: LoRA Fine-tuning with NeMo\n",
    "\n",
    "This notebook demonstrates how to fine-tune Llama 3.1 8B Instruct using LoRA (Low-Rank Adaptation) with NVIDIA NeMo framework.\n",
    "\n",
    "## What is LoRA?\n",
    "\n",
    "LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning method that:\n",
    "- Adds trainable low-rank matrices to frozen model weights\n",
    "- Reduces memory requirements by 90%+\n",
    "- Enables fine-tuning large models on consumer GPUs\n",
    "- Produces small adapter files (~100-500MB for 8B models)\n",
    "\n",
    "The focus of this workshop is not the specifics of LoRA, but to actually give everyone an guide on how to carry out the process of tuning your model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT: NeMo Framework Setup\n",
    "\n",
    "This notebook requires the NVIDIA NeMo framework for LoRA training. We'll use the cloned NeMo repository to access the necessary training scripts.\n",
    "\n",
    "**NeMo Compatibility**: \n",
    "- The downloaded model uses standard NeMo format (.nemo file)\n",
    "- The training scripts work directly without any modifications\n",
    "\n",
    "**Training Experience**: In this workshop, you'll train your own LoRA adapter from scratch! This gives you hands-on experience with:\n",
    "- Setting up training data\n",
    "- Configuring LoRA parameters\n",
    "- Running the actual training\n",
    "- Testing your custom adapter\n",
    "\n",
    "The training process takes approximately 5-10 minutes for our small example dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone NeMo Repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell downloads the NVIDIA NeMo framework (if not already present) and verifies that the required training scripts are available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone NeMo repository if not already present\n",
    "import os\n",
    "\n",
    "# Use relative path for NeMo\n",
    "nemo_path = './NeMo'\n",
    "\n",
    "if not os.path.exists(nemo_path):\n",
    "    print(\"Cloning NeMo repository...\")\n",
    "    !git clone https://github.com/NVIDIA/NeMo.git {nemo_path}\n",
    "    print(\"NeMo repository cloned successfully!\")\n",
    "else:\n",
    "    print(\"NeMo repository already exists.\")\n",
    "    \n",
    "# Verify the training scripts exist\n",
    "nemo_scripts = [\n",
    "    f'{nemo_path}/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py',\n",
    "    f'{nemo_path}/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py',\n",
    "    f'{nemo_path}/scripts/nlp_language_modeling/merge_lora_weights/merge.py'\n",
    "]\n",
    "\n",
    "print(\"\\nChecking for required NeMo scripts:\")\n",
    "for script in nemo_scripts:\n",
    "    if os.path.exists(script):\n",
    "        print(f\"‚úì Found: {os.path.basename(script)}\")\n",
    "    else:\n",
    "        print(f\"‚úó Missing: {script}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install jsonlines transformers omegaconf pytorch-lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell imports the required Python libraries for LoRA training and verifies that PyTorch can access the GPU, displaying the GPU name and available memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the directories needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories\n",
    "os.makedirs(\"lora_tutorial/data\", exist_ok=True)\n",
    "os.makedirs(\"lora_tutorial/models\", exist_ok=True)\n",
    "os.makedirs(\"lora_tutorial/configs\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Time to create our training data! \n",
    "\n",
    "We're creating a customer support AI, but here's the key insight: we're not teaching it WHAT customer support is - Llama already knows that. We're teaching it HOW to do customer support in YOUR specific style.\n",
    "\n",
    "Look at these examples:\n",
    "- Notice the consistent, professional tone\n",
    "- See how each response acknowledges the issue first\n",
    "- Watch how we always offer a clear next step\n",
    "\n",
    "With just 5 examples, we can dramatically change how the model responds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample dataset for customer support fine-tuning\n",
    "training_data = [\n",
    "    {\n",
    "        \"input\": \"User: My order hasn't arrived yet. Order number is 12345.\\\\n\\\\nAssistant:\",\n",
    "        \"output\": \"I apologize for the delay with your order #12345. Let me check the status for you right away. I'll need to verify some details first to ensure your privacy and security.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"User: How do I reset my password?\\\\n\\\\nAssistant:\",\n",
    "        \"output\": \"I'd be happy to help you reset your password. For security, please click on 'Forgot Password' on the login page, enter your email address, and follow the instructions sent to your inbox.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"User: What is your return policy?\\\\n\\\\nAssistant:\",\n",
    "        \"output\": \"Our return policy allows returns within 30 days of purchase with original receipt. Items must be in original condition with tags attached. Refunds are processed within 5-7 business days.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"User: I received a damaged product. What should I do?\\\\n\\\\nAssistant:\",\n",
    "        \"output\": \"I'm sorry to hear you received a damaged product. Please take photos of the damage and packaging, then contact us with your order number. We'll arrange a replacement or refund immediately.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"User: Do you offer international shipping?\\\\n\\\\nAssistant:\",\n",
    "        \"output\": \"Yes, we offer international shipping to over 50 countries. Shipping rates and delivery times vary by destination. You can check availability and costs at checkout.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save training data\n",
    "with jsonlines.open('lora_tutorial/data/train.jsonl', 'w') as writer:\n",
    "    writer.write_all(training_data)\n",
    "\n",
    "# Create validation data (smaller subset)\n",
    "val_data = training_data[:2]\n",
    "with jsonlines.open('lora_tutorial/data/val.jsonl', 'w') as writer:\n",
    "    writer.write_all(val_data)\n",
    "\n",
    "print(f\"Created {len(training_data)} training examples\")\n",
    "print(f\"Created {len(val_data)} validation examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Prerequisites Before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify prerequisites before training\n",
    "import os\n",
    "import glob\n",
    "\n",
    "print(\"üîç Checking prerequisites for training...\\n\")\n",
    "\n",
    "# Check if NeMo is cloned - use relative path\n",
    "nemo_path = \"./NeMo\"\n",
    "if os.path.exists(nemo_path):\n",
    "    print(\"‚úÖ NeMo repository found\")\n",
    "else:\n",
    "    print(\"‚ùå NeMo repository not found! Please run cell 2 to clone NeMo.\")\n",
    "\n",
    "# Check if training scripts exist\n",
    "training_script = f\"{nemo_path}/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py\"\n",
    "if os.path.exists(training_script):\n",
    "    print(\"‚úÖ Training script found\")\n",
    "else:\n",
    "    print(\"‚ùå Training script not found!\")\n",
    "\n",
    "# Check if model is downloaded - look in subdirectories since NGC creates them\n",
    "model_files = glob.glob(\"lora_tutorial/models/llama-3_1-8b-instruct/**/*.nemo\", recursive=True)\n",
    "if model_files:\n",
    "    model_path = model_files[0]  # Use the first .nemo file found\n",
    "    # Check if it's a complete model (>10GB)\n",
    "    size_gb = os.path.getsize(model_path) / (1024**3)\n",
    "    if size_gb > 10:\n",
    "        print(\"‚úÖ Llama 3.1 8B model found\")\n",
    "        print(f\"\\nüìÅ Model Information:\")\n",
    "        print(f\"   Path: {model_path}\")\n",
    "        print(f\"   Size: {size_gb:.2f} GB\")\n",
    "        print(f\"   Format: Standard NeMo checkpoint (.nemo)\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Incomplete model found ({size_gb:.1f} GB)\")\n",
    "        print(\"   Please re-run the download in 00_Workshop_Setup.ipynb\")\n",
    "else:\n",
    "    print(\"‚ùå Model not found! Please run notebook 00_Workshop_Setup.ipynb first\")\n",
    "\n",
    "# Check if training data exists\n",
    "if os.path.exists(\"lora_tutorial/data/train.jsonl\"):\n",
    "    print(\"‚úÖ Training data found\")\n",
    "else:\n",
    "    print(\"‚ùå Training data not found! Please run the data preparation cells\")\n",
    "\n",
    "print(\"\\nüéØ Ready to train!\" if all([\n",
    "    os.path.exists(nemo_path),\n",
    "    os.path.exists(training_script),\n",
    "    len(model_files) > 0 and os.path.getsize(model_files[0]) / (1024**3) > 10,  # Check complete model\n",
    "    os.path.exists(\"lora_tutorial/data/train.jsonl\")\n",
    "]) else \"\\n‚ö†Ô∏è Please fix the issues above before training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run LoRA Training\n",
    "\n",
    "### Actually Run the Training! üöÄ\n",
    "\n",
    "This is the exciting part - you'll train your own LoRA adapter! \n",
    "\n",
    "**What will happen:**\n",
    "1. The model will load from the .nemo checkpoint (takes ~30 seconds)\n",
    "2. Training will run for 50 steps (~5-10 minutes)\n",
    "3. Checkpoints will be saved every 25 steps\n",
    "4. A final LoRA adapter will be exported as a .nemo file\n",
    "\n",
    "**Note about warnings**: You'll see many warnings about missing configuration fields - these are normal and can be ignored. They appear because NeMo supports many optional features that aren't used in this training.\n",
    "\n",
    "Let's train your custom model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell runs the LoRA fine-tuning process, training Llama 3.1 8B on the customer support dataset for 50 steps using a single GPU with LoRA adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Actually run the LoRA training!\n",
    "\n",
    "# Find the model file dynamically (NGC creates subdirectories)\n",
    "MODEL_DIR=\"lora_tutorial/models/llama-3_1-8b-instruct\"\n",
    "MODEL=$(find \"$MODEL_DIR\" -name \"*.nemo\" -type f | head -1)\n",
    "\n",
    "if [ -z \"$MODEL\" ]; then\n",
    "    echo \"ERROR: No .nemo model file found in $MODEL_DIR\"\n",
    "    echo \"Please run notebook 00_Workshop_Setup.ipynb first to download the model\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "TRAIN_DS=\"[lora_tutorial/data/train.jsonl]\"\n",
    "VALID_DS=\"[lora_tutorial/data/val.jsonl]\"\n",
    "\n",
    "# Use relative path to NeMo\n",
    "NEMO_PATH=\"./NeMo\"\n",
    "\n",
    "echo \"‚úÖ Found Llama 3.1 8B model at $MODEL\"\n",
    "\n",
    "# Run training with NeMo\n",
    "torchrun --nproc_per_node=1 \\\n",
    "\"${NEMO_PATH}/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py\" \\\n",
    "    exp_manager.exp_dir=lora_tutorial/experiments \\\n",
    "    exp_manager.name=customer_support_lora \\\n",
    "    trainer.devices=1 \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    trainer.precision=bf16-mixed \\\n",
    "    trainer.val_check_interval=0.5 \\\n",
    "    trainer.max_steps=100 \\\n",
    "    model.megatron_amp_O2=True \\\n",
    "    ++model.mcore_gpt=True \\\n",
    "    model.tensor_model_parallel_size=1 \\\n",
    "    model.pipeline_model_parallel_size=1 \\\n",
    "    model.micro_batch_size=1 \\\n",
    "    model.global_batch_size=2 \\\n",
    "    model.restore_from_path=${MODEL} \\\n",
    "    model.data.train_ds.file_names=${TRAIN_DS} \\\n",
    "    model.data.train_ds.concat_sampling_probabilities=[1.0] \\\n",
    "    model.data.validation_ds.file_names=${VALID_DS} \\\n",
    "    model.peft.peft_scheme=lora \\\n",
    "    model.peft.lora_tuning.target_modules=[attention_qkv] \\\n",
    "    model.peft.lora_tuning.adapter_dim=32 \\\n",
    "    model.peft.lora_tuning.adapter_dropout=0.1 \\\n",
    "    model.optim.lr=5e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verify Training Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Look at what we've created! Let me break down these files:\n",
    "\n",
    "1. **customer_support_lora.nemo** (21MB) - This is your golden ticket! Your custom AI adapter\n",
    "2. **The .ckpt files** (147MB each) - Full training checkpoints with optimizer states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if training created the LoRA adapter\n",
    "!ls -la ./lora_tutorial/experiments/customer_support_lora*/checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've successfully:\n",
    "- ‚úÖ Set up the NeMo training environment\n",
    "- ‚úÖ Created training data for your custom task\n",
    "- ‚úÖ Configured LoRA parameters for efficient training\n",
    "- ‚úÖ Trained your own LoRA adapter on Llama 3.1 8B\n",
    "\n",
    "Your LoRA adapter is now ready to be deployed with NVIDIA NIM in the next notebook!\n",
    "\n",
    "**Next Steps**: \n",
    "- Open `04_Deploy_LoRA_with_NIM.ipynb` to deploy your custom model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
