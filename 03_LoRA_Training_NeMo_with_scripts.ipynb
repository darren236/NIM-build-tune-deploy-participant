{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to download the llama 3.2 1b instruct model (nemo checkpt) from ngc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "🎤 **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Welcome to the most transformative part of our journey - LoRA fine-tuning! This is where you go from using someone else's AI to creating YOUR OWN specialized AI.\n",
    "\n",
    "Let me start with a real story. A Fortune 500 company came to us with a problem. They loved Llama 3 70B but needed it to understand their internal jargon - thousands of product codes, technical terms, and specific procedures. \n",
    "\n",
    "The traditional solution? Fine-tune the entire 70B parameter model. That would require:\n",
    "- 8 H100 GPUs ($300,000+ hardware)\n",
    "- 2 weeks of training time  \n",
    "- Machine learning PhD to manage it\n",
    "- $50,000+ in electricity\n",
    "\n",
    "Their budget? One RTX 4090 and a week.\n",
    "\n",
    "Enter LoRA - Low-Rank Adaptation. Instead of training all 70 billion parameters, LoRA adds small 'adapter' matrices that modify the model's behavior. Imagine it like putting specialized glasses on the model - it sees everything through your custom lens.\n",
    "\n",
    "The results for that company?\n",
    "- Trained on 1 RTX 4090\n",
    "- 6 hours total time\n",
    "- Junior developer managed it\n",
    "- Under $100 in costs\n",
    "- Model performed BETTER than full fine-tuning for their use case\n",
    "\n",
    "Today, I'll show you exactly how to do this. By the end, you'll be able to create custom AI models tailored to your exact needs!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Part 3: LoRA Fine-tuning with NeMo\n",
    "\n",
    "This notebook demonstrates how to fine-tune models using LoRA (Low-Rank Adaptation) with NVIDIA NeMo framework.\n",
    "\n",
    "## What is LoRA?\n",
    "\n",
    "LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning method that:\n",
    "- Adds trainable low-rank matrices to frozen model weights\n",
    "- Reduces memory requirements by 90%+\n",
    "- Enables fine-tuning large models on consumer GPUs\n",
    "- Produces small adapter files (~10-100MB vs full model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "🎤 **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Let's set up our environment for LoRA training. The requirements are surprisingly modest compared to full fine-tuning.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. Setup Environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "🎤 **PRESENTER SCRIPT:**\n",
    "\n",
    "\"We need a few key packages for LoRA training. Let me explain each one:\n",
    "\n",
    "- `jsonlines`: For handling our training data format\n",
    "- `transformers`: HuggingFace's library, useful for tokenization\n",
    "- `omegaconf`: YAML configuration management (very clean!)\n",
    "- `pytorch-lightning`: Handles distributed training, logging, checkpoints\n",
    "\n",
    "[RUN THE CELL]\n",
    "\n",
    "Notice we're NOT installing the full NeMo framework for this demo. In production, you'd use NeMo for its optimized training loops, but these packages are enough to understand the concepts.\n",
    "\n",
    "While this installs, let me mention - LoRA was invented by Microsoft researchers in 2021. In just 2 years, it's revolutionized how we customize language models. The paper has 3000+ citations!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: jsonlines in /usr/local/lib/python3.10/dist-packages (4.0.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
      "Requirement already satisfied: omegaconf in /usr/local/lib/python3.10/dist-packages (2.2.3)\n",
      "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (23.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf) (4.9.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.3.0a0+ebedce2)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2025.5.1)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.4.0.post0)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.14.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (0.11.3.post0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->pytorch-lightning) (68.2.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (3.1.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.6.15)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->pytorch-lightning) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->pytorch-lightning) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install NeMo (if not already installed)\n",
    "# Note: This should be run in the NeMo directory\n",
    "# !cd /root/verb-workspace/NeMo && pip install -e \".[all]\"\n",
    "\n",
    "# For this tutorial, we'll install minimal requirements\n",
    "!pip install jsonlines transformers omegaconf pytorch-lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "🎤 **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Let's check our training hardware:\n",
    "\n",
    "[RUN THE CELL]\n",
    "\n",
    "For LoRA training, here's what you can accomplish with different GPUs:\n",
    "\n",
    "**RTX 4090 (24GB)**:\n",
    "- Llama 3.1 8B: Full LoRA training ✓\n",
    "- Llama 2 13B: LoRA with gradient checkpointing ✓\n",
    "- Llama 3.1 70B: LoRA with quantization ✓\n",
    "\n",
    "**A100 40GB**:\n",
    "- All of the above plus...\n",
    "- Llama 3.1 70B: Full LoRA training ✓\n",
    "- Multiple LoRA adapters simultaneously ✓\n",
    "\n",
    "**Consumer GPUs (16GB)**:\n",
    "- Llama 3.1 8B: LoRA with small batch sizes ✓\n",
    "- Mistral 7B: Full LoRA training ✓\n",
    "\n",
    "The memory formula: \n",
    "- Base model (frozen): ~2 bytes per parameter\n",
    "- LoRA adapters: ~0.02 bytes per parameter (1% of base)\n",
    "- Gradients & optimizer: ~8 bytes per trainable parameter\n",
    "\n",
    "So for Llama 3.1 8B:\n",
    "- Base: 16GB\n",
    "- LoRA: 160MB  \n",
    "- Training overhead: ~1.3GB\n",
    "- Total: ~18GB (fits in 24GB GPU!)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.0a0+ebedce2\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A100-SXM4-80GB\n",
      "GPU memory: 84.97 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "🎤 **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Let's organize our workspace professionally:\n",
    "\n",
    "[RUN THE CELL]\n",
    "\n",
    "Good project structure is crucial:\n",
    "- `data/`: Training and validation datasets\n",
    "- `models/`: Saved checkpoints and final models\n",
    "- `configs/`: YAML configurations for experiments\n",
    "\n",
    "In a real project, you'd also have:\n",
    "- `logs/`: TensorBoard logs\n",
    "- `scripts/`: Training and evaluation scripts\n",
    "- `results/`: Metrics and analysis\n",
    "- `tests/`: Unit tests for data processing\n",
    "\n",
    "Organization pays dividends when you're running multiple experiments!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories\n",
    "os.makedirs(\"lora_tutorial/data\", exist_ok=True)\n",
    "os.makedirs(\"lora_tutorial/models\", exist_ok=True)\n",
    "os.makedirs(\"lora_tutorial/configs\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Prepare Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "🎤 **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Now for the SECRET SAUCE - your training data. This is what makes your model unique. Let me create a customer service dataset as an example:\n",
    "\n",
    "[RUN THE CELL]\n",
    "\n",
    "Look at this data carefully. Each example has:\n",
    "- `input`: The customer's question with context\n",
    "- `output`: The EXACT response you want\n",
    "\n",
    "Key insights for great training data:\n",
    "1. **Quality > Quantity**: 1,000 excellent examples beat 100,000 mediocre ones\n",
    "2. **Diversity**: Cover edge cases, different phrasings, various scenarios  \n",
    "3. **Consistency**: Same style, tone, format across examples\n",
    "4. **Realism**: Use actual customer queries if possible\n",
    "\n",
    "For this demo, we have 5 examples. In production:\n",
    "- Minimum: 500-1,000 examples\n",
    "- Sweet spot: 5,000-10,000 examples\n",
    "- Diminishing returns: >50,000 examples\n",
    "\n",
    "Pro tip: Start small! Train with 100 examples first, test the model, identify gaps, add more targeted examples. Iterative improvement beats massive datasets.\n",
    "\n",
    "The JSONL format is perfect because:\n",
    "- Each line is independent (parallelizable)\n",
    "- Streamable for large datasets\n",
    "- Human-readable for debugging\n",
    "- Standard format tools understand\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 5 training examples\n",
      "Created 2 validation examples\n"
     ]
    }
   ],
   "source": [
    "# Create a sample dataset for customer support fine-tuning\n",
    "training_data = [\n",
    "    {\n",
    "        \"input\": \"User: My order hasn't arrived yet. Order number is 12345.\\n\\nAssistant:\",\n",
    "        \"output\": \"I apologize for the delay with your order #12345. Let me check the status for you right away. I'll need to verify some details first to ensure your privacy and security.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"User: How do I reset my password?\\n\\nAssistant:\",\n",
    "        \"output\": \"I'd be happy to help you reset your password. For security, please click on 'Forgot Password' on the login page, enter your email address, and follow the instructions sent to your inbox.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"User: What is your return policy?\\n\\nAssistant:\",\n",
    "        \"output\": \"Our return policy allows returns within 30 days of purchase with original receipt. Items must be in original condition with tags attached. Refunds are processed within 5-7 business days.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"User: I received a damaged product. What should I do?\\n\\nAssistant:\",\n",
    "        \"output\": \"I'm sorry to hear you received a damaged product. Please take photos of the damage and packaging, then contact us with your order number. We'll arrange a replacement or refund immediately.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"User: Do you offer international shipping?\\n\\nAssistant:\",\n",
    "        \"output\": \"Yes, we offer international shipping to over 50 countries. Shipping rates and delivery times vary by destination. You can check availability and costs at checkout.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save training data\n",
    "with jsonlines.open('lora_tutorial/data/train.jsonl', 'w') as writer:\n",
    "    writer.write_all(training_data)\n",
    "\n",
    "# Create validation data (smaller subset)\n",
    "val_data = training_data[:2]\n",
    "with jsonlines.open('lora_tutorial/data/val.jsonl', 'w') as writer:\n",
    "    writer.write_all(val_data)\n",
    "\n",
    "print(f\"Created {len(training_data)} training examples\")\n",
    "print(f\"Created {len(val_data)} validation examples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding LoRA Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎤 **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Let me show you how LoRA actually works under the hood. This is a simplified implementation for educational purposes:\n",
    "\n",
    "[RUN THE CELL]\n",
    "\n",
    "WOW! Look at those numbers:\n",
    "- Original layer: 16,777,216 parameters\n",
    "- LoRA adaptation: 262,144 parameters  \n",
    "- Reduction: 98.4%!\n",
    "\n",
    "Here's the mathematical magic:\n",
    "- Original: Y = X × W (where W is 4096×4096)\n",
    "- LoRA: Y = X × W + X × A × B × (α/r)\n",
    "  - A is 4096×32 (down-projection)\n",
    "  - B is 32×4096 (up-projection)\n",
    "  - W remains frozen!\n",
    "\n",
    "The intuition: Instead of changing the entire highway (W), we add a small side road (A×B) that modifies traffic flow.\n",
    "\n",
    "Why this works:\n",
    "1. Neural networks have low intrinsic rank\n",
    "2. Most fine-tuning changes lie in a low-dimensional subspace\n",
    "3. We're learning the 'diff' not the whole model\n",
    "\n",
    "Real-world impact: Meta trains separate LoRA adapters for 100+ languages on the same base model. Each adapter is ~100MB instead of 140GB!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original layer parameters: 16,777,216\n",
      "LoRA parameters: 262,144\n",
      "Parameter reduction: 98.4%\n"
     ]
    }
   ],
   "source": [
    "# This is a simplified demo of what LoRA training looks like\n",
    "# In practice, you would use NeMo's training scripts\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"Simplified LoRA layer for demonstration\"\"\"\n",
    "    def __init__(self, in_features, out_features, rank=16, alpha=16):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        # LoRA decomposition: W = W0 + BA\n",
    "        self.lora_A = nn.Parameter(torch.randn(in_features, rank) * 0.01)\n",
    "        self.lora_B = nn.Parameter(torch.zeros(rank, out_features))\n",
    "        \n",
    "    def forward(self, x, base_weight):\n",
    "        # Original forward: y = xW\n",
    "        base_output = x @ base_weight\n",
    "        \n",
    "        # LoRA forward: y = xW + x(BA) * scaling\n",
    "        lora_output = (x @ self.lora_A @ self.lora_B) * self.scaling\n",
    "        \n",
    "        return base_output + lora_output\n",
    "\n",
    "# Demonstrate parameter efficiency\n",
    "in_features, out_features = 4096, 4096\n",
    "rank = 32\n",
    "\n",
    "# Original parameters\n",
    "original_params = in_features * out_features\n",
    "print(f\"Original layer parameters: {original_params:,}\")\n",
    "\n",
    "# LoRA parameters\n",
    "lora_params = (in_features * rank) + (rank * out_features)\n",
    "print(f\"LoRA parameters: {lora_params:,}\")\n",
    "\n",
    "# Reduction\n",
    "reduction = (1 - lora_params / original_params) * 100\n",
    "print(f\"Parameter reduction: {reduction:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create LoRA Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎤 **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Here's where we configure the LoRA training. This YAML file controls everything. Let me explain the critical parameters:\n",
    "\n",
    "**LoRA Specific Settings**:\n",
    "- `adapter_dim: 32`: The 'rank' of LoRA matrices. Think of this as 'capacity':\n",
    "  - 8: Minimal changes, fast training\n",
    "  - 16: Good for most tasks\n",
    "  - 32: Our choice - balanced\n",
    "  - 64+: Approaching full fine-tuning\n",
    "\n",
    "- `target_modules: [\"attention_qkv\"]`: Which layers to adapt:\n",
    "  - attention_qkv: Query, Key, Value matrices (most common)\n",
    "  - attention_dense: Output projection\n",
    "  - mlp_fc1/fc2: Feed-forward layers\n",
    "  \n",
    "- `adapter_dropout: 0.1`: Prevents overfitting\n",
    "\n",
    "**Training Settings**:\n",
    "- `max_epochs: 3`: LoRA trains fast, don't overdo it\n",
    "- `learning_rate: 5e-4`: 10x higher than full fine-tuning!\n",
    "- `batch_size: 2`: Limited by GPU memory\n",
    "\n",
    "**Key Insight**: We're training ~0.5% of parameters but getting 95% of the performance. That's the LoRA magic!\n",
    "\n",
    "[RUN THE CELL]\n",
    "\n",
    "This configuration is your experiment definition. In production, you'd:\n",
    "- Version control these configs\n",
    "- Track experiments with MLflow/W&B\n",
    "- Hyperparameter sweep key values\n",
    "- A/B test different configurations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved configuration to lora_tutorial/configs/lora_config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Create LoRA configuration\n",
    "lora_config = {\n",
    "    \"name\": \"customer_support_lora\",\n",
    "    \n",
    "    \"trainer\": {\n",
    "        \"devices\": 1,\n",
    "        \"accelerator\": \"gpu\",\n",
    "        \"num_nodes\": 1,\n",
    "        \"precision\": \"bf16\",\n",
    "        \"max_epochs\": 3,\n",
    "        \"max_steps\": 100,\n",
    "        \"val_check_interval\": 25,\n",
    "        \"enable_checkpointing\": True,\n",
    "        \"logger\": True,\n",
    "    },\n",
    "    \n",
    "    \"model\": {\n",
    "        \"restore_from_path\": \"lora_tutorial/models/llama-3.2-1b-instruct-nemo_v1.0/1b_instruct_nemo_bf16.nemo\",  # Update this\n",
    "        \n",
    "        \"peft\": {\n",
    "            \"peft_scheme\": \"lora\",\n",
    "            \"restore_from_path\": None,\n",
    "            \n",
    "            \"lora_tuning\": {\n",
    "                \"target_modules\": [\"attention_qkv\"],\n",
    "                \"adapter_dim\": 32,\n",
    "                \"adapter_dropout\": 0.1,\n",
    "                \"column_init_method\": \"xavier\",\n",
    "                \"row_init_method\": \"zero\",\n",
    "                \"layer_selection\": None,\n",
    "                \"weight_tying\": False,\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        \"data\": {\n",
    "            \"train_ds\": {\n",
    "                \"file_names\": [\"./lora_tutorial/data/train.jsonl\"],\n",
    "                \"global_batch_size\": 2,\n",
    "                \"micro_batch_size\": 1,\n",
    "                \"shuffle\": True,\n",
    "                \"num_workers\": 4,\n",
    "                \"pin_memory\": True,\n",
    "                \"max_seq_length\": 512,\n",
    "                \"min_seq_length\": 1,\n",
    "                \"drop_last\": False,\n",
    "                \"concat_sampling_probabilities\": [1.0],\n",
    "                \"prompt_template\": \"{input} {output}\",\n",
    "            },\n",
    "            \n",
    "            \"validation_ds\": {\n",
    "                \"file_names\": [\"./lora_tutorial/data/val.jsonl\"],\n",
    "                \"global_batch_size\": 2,\n",
    "                \"micro_batch_size\": 1,\n",
    "                \"shuffle\": False,\n",
    "                \"num_workers\": 4,\n",
    "                \"pin_memory\": True,\n",
    "                \"max_seq_length\": 512,\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        \"optim\": {\n",
    "            \"name\": \"fused_adam\",\n",
    "            \"lr\": 5e-4,\n",
    "            \"weight_decay\": 0.01,\n",
    "            \"betas\": [0.9, 0.999],\n",
    "            \"sched\": {\n",
    "                \"name\": \"CosineAnnealing\",\n",
    "                \"warmup_steps\": 10,\n",
    "                \"constant_steps\": 0,\n",
    "                \"min_lr\": 1e-5,\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"exp_manager\": {\n",
    "        \"explicit_log_dir\": \"./lora_tutorial/experiments\",\n",
    "        \"exp_dir\": None,\n",
    "        \"name\": \"customer_support_lora\",\n",
    "        \"create_checkpoint_callback\": True,\n",
    "        \"checkpoint_callback_params\": {\n",
    "            \"monitor\": \"val_loss\",\n",
    "            \"save_top_k\": 3,\n",
    "            \"mode\": \"min\",\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save configuration\n",
    "config_path = \"lora_tutorial/configs/lora_config.yaml\"\n",
    "OmegaConf.save(OmegaConf.create(lora_config), config_path)\n",
    "print(f\"Saved configuration to {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Script Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎤 **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Here's a complete training script using NeMo. This is production-ready code with proper abstractions:\n",
    "\n",
    "[WALK THROUGH THE SCRIPT]\n",
    "\n",
    "Key sections:\n",
    "\n",
    "**1. Configuration Loading**:\n",
    "- Loads our YAML config\n",
    "- Mergeable and overrideable\n",
    "\n",
    "**2. Trainer Setup**:\n",
    "- Handles distributed training\n",
    "- Automatic mixed precision\n",
    "- Checkpointing and logging\n",
    "\n",
    "**3. Model Loading**:\n",
    "- Loads pre-trained base model\n",
    "- Freezes original weights\n",
    "- Adds LoRA adapters\n",
    "\n",
    "**4. The Magic Moment**:\n",
    "```python\n",
    "model.add_adapter(LoraPEFTConfig(model_cfg))\n",
    "```\n",
    "This single line:\n",
    "- Identifies target modules\n",
    "- Inserts LoRA matrices\n",
    "- Sets up proper gradients\n",
    "- Configures optimization\n",
    "\n",
    "**5. Training Loop**:\n",
    "- Standard PyTorch Lightning\n",
    "- Automatic gradient accumulation\n",
    "- Learning rate scheduling\n",
    "\n",
    "[SAVE THE SCRIPT]\n",
    "\n",
    "To run this in practice:\n",
    "```bash\n",
    "python train_lora.py\n",
    "```\n",
    "\n",
    "Training time estimates:\n",
    "- 1,000 examples: 1-2 hours\n",
    "- 10,000 examples: 6-12 hours  \n",
    "- 100,000 examples: 2-5 days\n",
    "\n",
    "Compare to full fine-tuning: 10-100x faster!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created training script: lora_tutorial/train_lora.py\n",
      "\n",
      "To run training (requires NeMo and base model):\n",
      "python lora_tutorial/train_lora.py\n"
     ]
    }
   ],
   "source": [
    "# Create a training script template\n",
    "train_script = '''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "LoRA Training Script for NeMo\n",
    "Usage: python train_lora.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from nemo.collections.nlp.models.language_modeling.megatron_gpt_sft_model import MegatronGPTSFTModel\n",
    "from nemo.collections.nlp.parts.megatron_trainer_builder import MegatronTrainerBuilder\n",
    "from nemo.collections.nlp.parts.peft_config import LoraPEFTConfig\n",
    "from nemo.utils import logging\n",
    "from nemo.utils.exp_manager import exp_manager\n",
    "\n",
    "def main():\n",
    "    # Load config\n",
    "    cfg = OmegaConf.load(\"lora_tutorial/configs/lora_config.yaml\")\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = MegatronTrainerBuilder(cfg).create_trainer()\n",
    "    \n",
    "    # Setup experiment manager\n",
    "    exp_manager(trainer, cfg.get(\"exp_manager\", None))\n",
    "    \n",
    "    # Load base model and merge configs\n",
    "    model_cfg = MegatronGPTSFTModel.merge_cfg_with(\n",
    "        cfg.model.restore_from_path, \n",
    "        cfg\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    model = MegatronGPTSFTModel.restore_from(\n",
    "        cfg.model.restore_from_path, \n",
    "        model_cfg, \n",
    "        trainer=trainer\n",
    "    )\n",
    "    \n",
    "    # Add LoRA adapter\n",
    "    logging.info(\"Adding LoRA adapter...\")\n",
    "    model.add_adapter(LoraPEFTConfig(model_cfg))\n",
    "    \n",
    "    # Print parameter count\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    logging.info(f\"Total parameters: {total_params:,}\")\n",
    "    logging.info(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    logging.info(f\"Trainable %: {100 * trainable_params / total_params:.2f}%\")\n",
    "    \n",
    "    # Start training\n",
    "    logging.info(\"Starting LoRA training...\")\n",
    "    trainer.fit(model)\n",
    "    \n",
    "    logging.info(\"Training completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Save training script\n",
    "with open(\"lora_tutorial/train_lora.py\", \"w\") as f:\n",
    "    f.write(train_script)\n",
    "\n",
    "print(\"Created training script: lora_tutorial/train_lora.py\")\n",
    "print(\"\\nTo run training (requires NeMo and base model):\")\n",
    "print(\"python lora_tutorial/train_lora.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Fix Dependencies Issue\n",
    "\n",
    "There's a version mismatch with huggingface_hub. Let's fix it before running training:\n",
    "\n",
    "The root cause is that NeMo was developed with an older version of huggingface_hub (0.23.x) but your environment has a newer version (0.33.2) where ModelFilter has been removed. The downgrade should resolve this issue and allow the training to proceed normally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: 0.23.4\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting huggingface_hub==0.23.4\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting filelock (from huggingface_hub==0.23.4)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface_hub==0.23.4)\n",
      "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting packaging>=20.9 (from huggingface_hub==0.23.4)\n",
      "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pyyaml>=5.1 (from huggingface_hub==0.23.4)\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting requests (from huggingface_hub==0.23.4)\n",
      "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tqdm>=4.42.1 (from huggingface_hub==0.23.4)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-extensions>=3.7.4.3 (from huggingface_hub==0.23.4)\n",
      "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->huggingface_hub==0.23.4)\n",
      "  Downloading charset_normalizer-3.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->huggingface_hub==0.23.4)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->huggingface_hub==0.23.4)\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->huggingface_hub==0.23.4)\n",
      "  Downloading certifi-2025.6.15-py3-none-any.whl.metadata (2.4 kB)\n",
      "Downloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.6/402.6 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m313.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading packaging-25.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m248.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.2/751.2 kB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m271.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m239.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m275.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2025.6.15-py3-none-any.whl (157 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.7/157.7 kB\u001b[0m \u001b[31m298.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading charset_normalizer-3.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.5/149.5 kB\u001b[0m \u001b[31m292.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m272.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.8/129.8 kB\u001b[0m \u001b[31m282.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: urllib3, typing-extensions, tqdm, pyyaml, packaging, idna, fsspec, filelock, charset_normalizer, certifi, requests, huggingface_hub\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.5.0\n",
      "    Uninstalling urllib3-2.5.0:\n",
      "      Successfully uninstalled urllib3-2.5.0\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.14.0\n",
      "    Uninstalling typing_extensions-4.14.0:\n",
      "      Successfully uninstalled typing_extensions-4.14.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.67.1\n",
      "    Uninstalling tqdm-4.67.1:\n",
      "      Successfully uninstalled tqdm-4.67.1\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 6.0.2\n",
      "    Uninstalling PyYAML-6.0.2:\n",
      "      Successfully uninstalled PyYAML-6.0.2\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 25.0\n",
      "    Uninstalling packaging-25.0:\n",
      "      Successfully uninstalled packaging-25.0\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.10\n",
      "    Uninstalling idna-3.10:\n",
      "      Successfully uninstalled idna-3.10\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.5.1\n",
      "    Uninstalling fsspec-2025.5.1:\n",
      "      Successfully uninstalled fsspec-2025.5.1\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.18.0\n",
      "    Uninstalling filelock-3.18.0:\n",
      "      Successfully uninstalled filelock-3.18.0\n",
      "  Attempting uninstall: charset_normalizer\n",
      "    Found existing installation: charset-normalizer 3.4.2\n",
      "    Uninstalling charset-normalizer-3.4.2:\n",
      "      Successfully uninstalled charset-normalizer-3.4.2\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2025.6.15\n",
      "    Uninstalling certifi-2025.6.15:\n",
      "      Successfully uninstalled certifi-2025.6.15\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.4\n",
      "    Uninstalling requests-2.32.4:\n",
      "      Successfully uninstalled requests-2.32.4\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 0.23.4\n",
      "    Uninstalling huggingface-hub-0.23.4:\n",
      "      Successfully uninstalled huggingface-hub-0.23.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.33.16 requires docutils<0.17,>=0.10, but you have docutils 0.21.2 which is incompatible.\n",
      "cudf 23.12.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.2.1 which is incompatible.\n",
      "cugraph 23.12.0 requires dask-cuda==23.12.*, but you have dask-cuda 24.4.0 which is incompatible.\n",
      "cugraph 23.12.0 requires rapids-dask-dependency==23.12.*, but you have rapids-dask-dependency 24.4.1 which is incompatible.\n",
      "cugraph-service-server 23.12.0 requires dask-cuda==23.12.*, but you have dask-cuda 24.4.0 which is incompatible.\n",
      "cugraph-service-server 23.12.0 requires rapids-dask-dependency==23.12.*, but you have rapids-dask-dependency 24.4.1 which is incompatible.\n",
      "cuml 23.12.0 requires dask-cuda==23.12.*, but you have dask-cuda 24.4.0 which is incompatible.\n",
      "cuml 23.12.0 requires rapids-dask-dependency==23.12.*, but you have rapids-dask-dependency 24.4.1 which is incompatible.\n",
      "dask 2024.1.1 requires click>=8.1, but you have click 8.0.2 which is incompatible.\n",
      "dask-cudf 23.12.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.2.1 which is incompatible.\n",
      "dask-cudf 23.12.0 requires rapids-dask-dependency==23.12.*, but you have rapids-dask-dependency 24.4.1 which is incompatible.\n",
      "datasets 2.19.2 requires fsspec[http]<=2024.3.1,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\n",
      "peft 0.15.2 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.23.4 which is incompatible.\n",
      "spacy 3.7.2 requires typer<0.10.0,>=0.3.0, but you have typer 0.16.0 which is incompatible.\n",
      "tensorrt-llm 0.9.0 requires diffusers==0.15.0, but you have diffusers 0.29.2 which is incompatible.\n",
      "tensorrt-llm 0.9.0 requires nvidia-ammo~=0.7.0, but you have nvidia-ammo 0.0.0 which is incompatible.\n",
      "tensorrt-llm 0.9.0 requires pynvml>=11.5.0, but you have pynvml 11.4.1 which is incompatible.\n",
      "torch-tensorrt 2.3.0a0 requires tensorrt<8.7,>=8.6, but you have tensorrt 9.3.0.post12.dev1 which is incompatible.\n",
      "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.16.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed certifi-2025.6.15 charset_normalizer-3.4.2 filelock-3.18.0 fsspec-2025.5.1 huggingface_hub-0.23.4 idna-3.10 packaging-25.0 pyyaml-6.0.2 requests-2.32.4 tqdm-4.67.1 typing-extensions-4.14.1 urllib3-2.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Fixed huggingface_hub version. Now we can proceed with training.\n"
     ]
    }
   ],
   "source": [
    "# Fix the huggingface_hub version issue\n",
    "# The error is because NeMo expects a different version of huggingface_hub\n",
    "# Let's check current version and downgrade if needed\n",
    "\n",
    "!pip show huggingface_hub | grep Version\n",
    "\n",
    "# Downgrade to a compatible version\n",
    "%pip install huggingface_hub==0.23.4 --force-reinstall\n",
    "\n",
    "print(\"\\nFixed huggingface_hub version. Now we can proceed with training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Actually Run the Training\n",
    "\n",
    "Now let's execute the training using NeMo's script directly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/python: can't open file '/root/verb-workspace/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py': [Errno 2] No such file or directory\n",
      "[2025-07-08 10:11:08,553] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 2) local_rank: 0 (pid: 74482) of binary: /usr/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 812, in main\n",
      "    run(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 803, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 135, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "/root/verb-workspace/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-07-08_10:11:08\n",
      "  host      : verb-workspace\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 2 (pid: 74482)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'\\n# Actually run the LoRA training!\\n# Note: We use NeMo\\'s script directly instead of the template we created\\n\\nMODEL=\"/root/verb-workspace/lora_tutorial/models/llama-3-8b-instruct-nemo_v1.0/8b_instruct_nemo_bf16.nemo\"\\nTRAIN_DS=\"[./lora_tutorial/data/train.jsonl]\"\\nVALID_DS=\"[./lora_tutorial/data/val.jsonl]\"\\n\\n# Run training with NeMo\\ntorchrun --nproc_per_node=1 \\\\\\n/root/verb-workspace/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \\\\\\n    exp_manager.exp_dir=./lora_tutorial/experiments \\\\\\n    exp_manager.name=customer_support_lora \\\\\\n    trainer.devices=1 \\\\\\n    trainer.num_nodes=1 \\\\\\n    trainer.precision=bf16-mixed \\\\\\n    trainer.val_check_interval=0.5 \\\\\\n    trainer.max_steps=50 \\\\\\n    model.megatron_amp_O2=True \\\\\\n    ++model.mcore_gpt=True \\\\\\n    model.tensor_model_parallel_size=1 \\\\\\n    model.pipeline_model_parallel_size=1 \\\\\\n    model.micro_batch_size=1 \\\\\\n    model.global_batch_size=2 \\\\\\n    model.restore_from_path=${MODEL} \\\\\\n    model.data.train_ds.file_names=${TRAIN_DS} \\\\\\n    model.data.train_ds.concat_sampling_probabilities=[1.0] \\\\\\n    model.data.validation_ds.file_names=${VALID_DS} \\\\\\n    model.peft.peft_scheme=lora \\\\\\n    model.peft.lora_tuning.target_modules=[attention_qkv] \\\\\\n    model.peft.lora_tuning.adapter_dim=32 \\\\\\n    model.peft.lora_tuning.adapter_dropout=0.1 \\\\\\n    model.optim.lr=5e-4\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Actually run the LoRA training!\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Note: We use NeMo\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43ms script directly instead of the template we created\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mMODEL=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/root/verb-workspace/lora_tutorial/models/llama-3-8b-instruct-nemo_v1.0/8b_instruct_nemo_bf16.nemo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mTRAIN_DS=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m[./lora_tutorial/data/train.jsonl]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mVALID_DS=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m[./lora_tutorial/data/val.jsonl]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Run training with NeMo\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mtorchrun --nproc_per_node=1 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m/root/verb-workspace/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    exp_manager.exp_dir=./lora_tutorial/experiments \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    exp_manager.name=customer_support_lora \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    trainer.devices=1 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    trainer.num_nodes=1 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    trainer.precision=bf16-mixed \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    trainer.val_check_interval=0.5 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    trainer.max_steps=50 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.megatron_amp_O2=True \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    ++model.mcore_gpt=True \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.tensor_model_parallel_size=1 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.pipeline_model_parallel_size=1 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.micro_batch_size=1 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.global_batch_size=2 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.restore_from_path=$\u001b[39;49m\u001b[38;5;132;43;01m{MODEL}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.data.train_ds.file_names=$\u001b[39;49m\u001b[38;5;132;43;01m{TRAIN_DS}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.data.train_ds.concat_sampling_probabilities=[1.0] \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.data.validation_ds.file_names=$\u001b[39;49m\u001b[38;5;132;43;01m{VALID_DS}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.peft.peft_scheme=lora \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.peft.lora_tuning.target_modules=[attention_qkv] \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.peft.lora_tuning.adapter_dim=32 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.peft.lora_tuning.adapter_dropout=0.1 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.optim.lr=5e-4\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:2517\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2515\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2516\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2517\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2519\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2520\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2521\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magics/script.py:154\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magics/script.py:314\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'\\n# Actually run the LoRA training!\\n# Note: We use NeMo\\'s script directly instead of the template we created\\n\\nMODEL=\"/root/verb-workspace/lora_tutorial/models/llama-3-8b-instruct-nemo_v1.0/8b_instruct_nemo_bf16.nemo\"\\nTRAIN_DS=\"[./lora_tutorial/data/train.jsonl]\"\\nVALID_DS=\"[./lora_tutorial/data/val.jsonl]\"\\n\\n# Run training with NeMo\\ntorchrun --nproc_per_node=1 \\\\\\n/root/verb-workspace/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \\\\\\n    exp_manager.exp_dir=./lora_tutorial/experiments \\\\\\n    exp_manager.name=customer_support_lora \\\\\\n    trainer.devices=1 \\\\\\n    trainer.num_nodes=1 \\\\\\n    trainer.precision=bf16-mixed \\\\\\n    trainer.val_check_interval=0.5 \\\\\\n    trainer.max_steps=50 \\\\\\n    model.megatron_amp_O2=True \\\\\\n    ++model.mcore_gpt=True \\\\\\n    model.tensor_model_parallel_size=1 \\\\\\n    model.pipeline_model_parallel_size=1 \\\\\\n    model.micro_batch_size=1 \\\\\\n    model.global_batch_size=2 \\\\\\n    model.restore_from_path=${MODEL} \\\\\\n    model.data.train_ds.file_names=${TRAIN_DS} \\\\\\n    model.data.train_ds.concat_sampling_probabilities=[1.0] \\\\\\n    model.data.validation_ds.file_names=${VALID_DS} \\\\\\n    model.peft.peft_scheme=lora \\\\\\n    model.peft.lora_tuning.target_modules=[attention_qkv] \\\\\\n    model.peft.lora_tuning.adapter_dim=32 \\\\\\n    model.peft.lora_tuning.adapter_dropout=0.1 \\\\\\n    model.optim.lr=5e-4\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Actually run the LoRA training!\n",
    "# Note: We use NeMo's script directly instead of the template we created\n",
    "\n",
    "MODEL=\"/root/verb-workspace/lora_tutorial/models/llama-3-8b-instruct-nemo_v1.0/8b_instruct_nemo_bf16.nemo\"\n",
    "TRAIN_DS=\"[./lora_tutorial/data/train.jsonl]\"\n",
    "VALID_DS=\"[./lora_tutorial/data/val.jsonl]\"\n",
    "\n",
    "# Run training with NeMo\n",
    "torchrun --nproc_per_node=1 \\\n",
    "/root/verb-workspace/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \\\n",
    "    exp_manager.exp_dir=./lora_tutorial/experiments \\\n",
    "    exp_manager.name=customer_support_lora \\\n",
    "    trainer.devices=1 \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    trainer.precision=bf16-mixed \\\n",
    "    trainer.val_check_interval=0.5 \\\n",
    "    trainer.max_steps=50 \\\n",
    "    model.megatron_amp_O2=True \\\n",
    "    ++model.mcore_gpt=True \\\n",
    "    model.tensor_model_parallel_size=1 \\\n",
    "    model.pipeline_model_parallel_size=1 \\\n",
    "    model.micro_batch_size=1 \\\n",
    "    model.global_batch_size=2 \\\n",
    "    model.restore_from_path=${MODEL} \\\n",
    "    model.data.train_ds.file_names=${TRAIN_DS} \\\n",
    "    model.data.train_ds.concat_sampling_probabilities=[1.0] \\\n",
    "    model.data.validation_ds.file_names=${VALID_DS} \\\n",
    "    model.peft.peft_scheme=lora \\\n",
    "    model.peft.lora_tuning.target_modules=[attention_qkv] \\\n",
    "    model.peft.lora_tuning.adapter_dim=32 \\\n",
    "    model.peft.lora_tuning.adapter_dropout=0.1 \\\n",
    "    model.optim.lr=5e-4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Why Baseline Metrics Might Not Show\n",
    "\n",
    "**Important Note**: The test metrics table might not appear in the baseline check because:\n",
    "\n",
    "1. **Generation vs Evaluation Mode**: \n",
    "   - `megatron_gpt_generate.py` is optimized for text generation\n",
    "   - It only calculates loss when it has the full context (during training)\n",
    "   - Without training, it focuses on generation only\n",
    "\n",
    "2. **No Training = No Loss Calculation**:\n",
    "   - Loss requires comparing predictions to ground truth token-by-token\n",
    "   - This happens naturally during training (teacher forcing)\n",
    "   - Pure inference/generation doesn't always compute this\n",
    "\n",
    "3. **Alternative Approaches**:\n",
    "   - Run training for 0 steps to get initial loss\n",
    "   - Use a dedicated evaluation script\n",
    "   - Compare generated text quality instead of numerical metrics\n",
    "\n",
    "**What to do**: Focus on comparing the generated responses rather than loss values for baseline!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎤 **PRESENTER SCRIPT:**\n",
    " \n",
    "\"Now let's verify that our LoRA training was successful by checking the output files.\n",
    " \n",
    "As we can see, the training has created three important files:\n",
    " \n",
    "**customer_support_lora.nemo** (21MB) - This is the exported LoRA adapter in NeMo format.\n",
    "It contains just the LoRA weights and configuration, which is why it's so small compared\n",
    "to the full model. This is what we'll deploy with NIM.\n",
    " \n",
    "2. **Two checkpoint files** (147MB each) - These are the full training checkpoints that include:\n",
    "- The LoRA adapter weights\n",
    "- Optimizer state\n",
    "- Training metadata\n",
    "- Model configuration\n",
    "    \n",
    "The checkpoint files are larger because they contain everything needed to resume training.\n",
    "Notice they're named with the validation loss (0.000) and training step (50).\n",
    " \n",
    "The fact that we have a 21MB .nemo file confirms our LoRA adapter was successfully created.\n",
    "This small file size is one of the key advantages of LoRA - we've adapted a 15GB model\n",
    "with just 21MB of additional weights!\n",
    " \n",
    "In the next section, we'll deploy this adapter with NIM to serve our fine-tuned model.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 307504\n",
      "drwxr-xr-x 2 root root      4096 Jul  4 05:03  .\n",
      "drwxr-xr-x 4 root root      4096 Jul  4 05:03  ..\n",
      "-rw-r--r-- 1 root root  21012480 Jul  4 05:03  customer_support_lora.nemo\n",
      "-rw-r--r-- 1 root root 146929774 Jul  4 05:03 'megatron_gpt_peft_lora_tuning--validation_loss=0.000-step=50-consumed_samples=100.0-last.ckpt'\n",
      "-rw-r--r-- 1 root root 146929774 Jul  4 05:03 'megatron_gpt_peft_lora_tuning--validation_loss=0.000-step=50-consumed_samples=100.0.ckpt'\n"
     ]
    }
   ],
   "source": [
    "# Check if training created the LoRA adapter\n",
    "!ls -la ./lora_tutorial/experiments/customer_support_lora*/checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎤 **PRESENTER SCRIPT:**\n",
    "\n",
    "\"After training, how do we use our LoRA model? This script shows the inference process:\n",
    "\n",
    "[EXPLAIN THE CODE]\n",
    "\n",
    "The beautiful thing about LoRA inference:\n",
    "\n",
    "**Option 1: Dynamic Loading** (shown here)\n",
    "- Keep base model and adapter separate\n",
    "- Load adapter on-demand\n",
    "- Switch adapters at runtime\n",
    "- One base model, many behaviors\n",
    "\n",
    "**Option 2: Merged Deployment**\n",
    "- Merge LoRA weights into base model\n",
    "- Single model file\n",
    "- Slightly faster inference\n",
    "- Less flexible\n",
    "\n",
    "The code flow:\n",
    "1. Load base model (cached, fast)\n",
    "2. Load LoRA adapter (tiny file, instant)\n",
    "3. Freeze everything (inference mode)\n",
    "4. Generate responses normally\n",
    "\n",
    "Look at those test prompts - they match our training data domain. The model should respond in our customer service style!\n",
    "\n",
    "[SAVE THE SCRIPT]\n",
    "\n",
    "Pro tip: You can load multiple LoRA adapters and interpolate between them:\n",
    "```python\n",
    "model.load_adapter(lora1, weight=0.7)\n",
    "model.load_adapter(lora2, weight=0.3)\n",
    "```\n",
    "This blends behaviors - like mixing painting styles!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created inference script: lora_tutorial/inference_lora.py\n"
     ]
    }
   ],
   "source": [
    "# Create inference script template\n",
    "inference_script = '''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "LoRA Inference Script\n",
    "Usage: python inference_lora.py\n",
    "\"\"\"\n",
    "\n",
    "from nemo.collections.nlp.models.language_modeling.megatron_gpt_sft_model import MegatronGPTSFTModel\n",
    "from nemo.collections.nlp.parts.peft_config import LoraPEFTConfig\n",
    "import torch\n",
    "\n",
    "def load_model_with_lora(base_model_path, lora_checkpoint_path):\n",
    "    \"\"\"Load base model and LoRA adapter\"\"\"\n",
    "    # Load base model\n",
    "    model = MegatronGPTSFTModel.restore_from(\n",
    "        base_model_path,\n",
    "        trainer=None,\n",
    "        map_location='cuda:0'\n",
    "    )\n",
    "    \n",
    "    # Load LoRA adapter\n",
    "    model.load_adapters(lora_checkpoint_path, LoraPEFTConfig(model.cfg))\n",
    "    model.freeze()\n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def generate_response(model, prompt, max_length=200):\n",
    "    \"\"\"Generate response using the model\"\"\"\n",
    "    inputs = model.tokenizer(prompt, return_tensors='pt').to('cuda')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9\n",
    "        )\n",
    "    \n",
    "    return model.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    model = load_model_with_lora(\n",
    "        \"path/to/base/model.nemo\",\n",
    "        \"path/to/lora/checkpoint.nemo\"\n",
    "    )\n",
    "    \n",
    "    # Test prompts\n",
    "    test_prompts = [\n",
    "        \"User: My package is damaged. What should I do?\\\\n\\\\nAssistant:\",\n",
    "        \"User: How do I track my order?\\\\n\\\\nAssistant:\",\n",
    "    ]\n",
    "    \n",
    "    for prompt in test_prompts:\n",
    "        response = generate_response(model, prompt)\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Response: {response}\")\n",
    "        print(\"-\" * 50)\n",
    "'''\n",
    "\n",
    "# Save inference script\n",
    "with open(\"lora_tutorial/inference_lora.py\", \"w\") as f:\n",
    "    f.write(inference_script)\n",
    "\n",
    "print(\"Created inference script: lora_tutorial/inference_lora.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Run Inference with the Trained LoRA\n",
    "\n",
    "Let's test our fine-tuned model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created test file with 2 examples\n"
     ]
    }
   ],
   "source": [
    "# First, create a test file with a few examples\n",
    "test_examples = [\n",
    "    {\n",
    "        \"input\": \"User: My package is damaged. What should I do?\\n\\nAssistant:\",\n",
    "        \"output\": \"I'm sorry to hear you received a damaged product. Please take photos of the damage and packaging, then contact us with your order number. We'll arrange a replacement or refund immediately.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"User: How do I track my order?\\n\\nAssistant:\",\n",
    "        \"output\": \"You can track your order by logging into your account and clicking 'Order History', or use the tracking link in your confirmation email. The tracking number will show real-time updates.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "with jsonlines.open('lora_tutorial/data/test_small.jsonl', 'w') as writer:\n",
    "    writer.write_all(test_examples)\n",
    "    \n",
    "print(\"Created test file with 2 examples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-04 06:35:13 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-04 06:35:13 megatron_gpt_generate:127] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2025-07-04 06:35:13 megatron_gpt_generate:128] \n",
      "    name: megatron_gpt_peft_${model.peft.peft_scheme}_tuning\n",
      "    trainer:\n",
      "      devices: 1\n",
      "      accelerator: gpu\n",
      "      num_nodes: 1\n",
      "      precision: 16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: 9999\n",
      "      max_steps: 20000\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 200\n",
      "      gradient_clip_val: 1.0\n",
      "    exp_manager:\n",
      "      explicit_log_dir: null\n",
      "      exp_dir: null\n",
      "      name: ${name}\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "      resume_if_exists: true\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: validation_${model.data.test_ds.metric.name}\n",
      "        save_top_k: 1\n",
      "        mode: max\n",
      "        save_nemo_on_train_end: true\n",
      "        filename: ${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${model.tensor_model_parallel_size}\n",
      "        always_save_nemo: true\n",
      "        save_best_model: false\n",
      "    model:\n",
      "      seed: 1234\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      global_batch_size: 1\n",
      "      micro_batch_size: 1\n",
      "      restore_from_path: /root/verb-workspace/lora_tutorial/models/llama-3-8b-instruct-nemo_v1.0/8b_instruct_nemo_bf16.nemo\n",
      "      resume_from_checkpoint: null\n",
      "      save_nemo_on_validation_end: true\n",
      "      sync_batch_comm: false\n",
      "      megatron_amp_O2: false\n",
      "      sequence_parallel: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      answer_only_loss: true\n",
      "      gradient_as_bucket_view: false\n",
      "      hidden_dropout: 0.0\n",
      "      attention_dropout: 0.0\n",
      "      ffn_dropout: 0.0\n",
      "      peft:\n",
      "        peft_scheme: adapter\n",
      "        restore_from_path: ./lora_tutorial/experiments/customer_support_lora/checkpoints/customer_support_lora.nemo\n",
      "        restore_from_ckpt:\n",
      "          checkpoint_dir: null\n",
      "          checkpoint_name: null\n",
      "        adapter_tuning:\n",
      "          type: parallel_adapter\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          norm_position: pre\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          norm_type: mixedfusedlayernorm\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        lora_tuning:\n",
      "          variant: nemo\n",
      "          target_modules:\n",
      "          - attention_qkv\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        p_tuning:\n",
      "          virtual_tokens: 10\n",
      "          bottleneck_dim: 1024\n",
      "          embedding_dim: 1024\n",
      "          init_std: 0.023\n",
      "        ia3_tuning:\n",
      "          layer_selection: null\n",
      "      data:\n",
      "        test_ds:\n",
      "          file_names:\n",
      "          - ./lora_tutorial/data/test_small.jsonl\n",
      "          names:\n",
      "          - customer_support\n",
      "          global_batch_size: 1\n",
      "          micro_batch_size: 1\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          context_key: input\n",
      "          label_key: output\n",
      "          add_eos: true\n",
      "          add_sep: false\n",
      "          add_bos: false\n",
      "          write_predictions_to_file: true\n",
      "          output_file_path_prefix: customer_support_lora\n",
      "          truncation_field: ${data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: '{input} {output}'\n",
      "          tokens_to_generate: 100\n",
      "          truncation_method: right\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "    inference:\n",
      "      greedy: true\n",
      "      top_k: 0\n",
      "      top_p: 0.9\n",
      "      temperature: 1.0\n",
      "      all_probs: false\n",
      "      repetition_penalty: 1.0\n",
      "      min_tokens_to_generate: 0\n",
      "      compute_logprob: false\n",
      "      outfile_path: output.txt\n",
      "      compute_attention_mask: true\n",
      "    server: false\n",
      "    port: 5555\n",
      "    web_server: false\n",
      "    share: true\n",
      "    username: test\n",
      "    password: test2\n",
      "    web_port: 9889\n",
      "    chat: false\n",
      "    chatbot_config:\n",
      "      value: false\n",
      "      attributes:\n",
      "      - name: Quality\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: quality\n",
      "        type: int\n",
      "        default: 4\n",
      "      - name: Toxicity\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: toxcity\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Humor\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: humor\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Creativity\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: creativity\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Violence\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: violence\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Helpfulness\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: helpfulness\n",
      "        type: int\n",
      "        default: 4\n",
      "      - name: Not_Appropriate\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: not_appropriate\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Language\n",
      "        choices:\n",
      "        - ar\n",
      "        - bg\n",
      "        - bn\n",
      "        - ca\n",
      "        - cs\n",
      "        - da\n",
      "        - de\n",
      "        - el\n",
      "        - en\n",
      "        - eo\n",
      "        - es\n",
      "        - eu\n",
      "        - fa\n",
      "        - fi\n",
      "        - fr\n",
      "        - gl\n",
      "        - he\n",
      "        - hu\n",
      "        - id\n",
      "        - it\n",
      "        - ja\n",
      "        - ko\n",
      "        - nb\n",
      "        - nl\n",
      "        - pl\n",
      "        - pt\n",
      "        - ro\n",
      "        - ru\n",
      "        - sk\n",
      "        - sv\n",
      "        - th\n",
      "        - tr\n",
      "        - uk\n",
      "        - vi\n",
      "        - zh\n",
      "        key: lang\n",
      "        type: list\n",
      "        default: en\n",
      "      user: User\n",
      "      assistant: Assistant\n",
      "      system: 'A chat between a curious human and an artificial intelligence assistant.\n",
      "        The assistant gives helpful, detailed, and polite answers to the human''s questions.\n",
      "    \n",
      "    \n",
      "        '\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-04 06:35:13 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
      "    \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "25-07-04 06:35:29 - PID:57266 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-04 06:35:29 megatron_init:263] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2025-07-04 06:35:29 megatron_init:269] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2025-07-04 06:35:29 megatron_init:274] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2025-07-04 06:35:29 megatron_init:277] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2025-07-04 06:35:29 megatron_init:285] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2025-07-04 06:35:29 megatron_init:288] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2025-07-04 06:35:29 megatron_init:289] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2025-07-04 06:35:29 megatron_init:296] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2025-07-04 06:35:29 megatron_init:297] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-07-04 06:35:29 megatron_init:306] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2025-07-04 06:35:29 megatron_init:310] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-07-04 06:35:29 megatron_init:311] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2025-07-04 06:35:29 megatron_init:331] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2025-07-04 06:35:29 megatron_init:343] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2025-07-04 06:35:29 megatron_init:349] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-07-04 06:35:29 megatron_init:350] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2025-07-04 06:35:29 megatron_init:351] All embedding group ranks: [[0]]\n",
      "[NeMo I 2025-07-04 06:35:29 megatron_init:352] Rank 0 has embedding rank: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-04 06:35:29 tokenizer_utils:178] Getting HuggingFace AutoTokenizer with pretrained_model_name: meta-llama/Meta-Llama-3-8B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-04 06:35:29 megatron_base_model:584] Padded vocab_size: 128256, original vocab_size: 128256, dummy tokens: 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:1158] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2025-07-04 06:35:29 megatron_base_model:556] The model: MegatronGPTSFTModel() does not have field.name: rotary_percent in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-04 06:35:47 dist_ckpt_io:95] Using ('zarr', 1) dist-ckpt save strategy.\n",
      "Loading distributed checkpoint with TensorStoreLoadShardedStrategy\n",
      "Loading distributed checkpoint directly on the GPU\n",
      "[NeMo I 2025-07-04 06:36:45 nlp_overrides:1180] Model MegatronGPTSFTModel was successfully restored from /root/verb-workspace/lora_tutorial/models/llama-3-8b-instruct-nemo_v1.0/8b_instruct_nemo_bf16.nemo.\n",
      "[NeMo I 2025-07-04 06:36:45 nlp_adapter_mixins:203] Before adding PEFT params:\n",
      "      | Name  | Type     | Params | Mode \n",
      "    -------------------------------------------\n",
      "    0 | model | GPTModel | 8.0 B  | train\n",
      "    -------------------------------------------\n",
      "    0         Trainable params\n",
      "    8.0 B     Non-trainable params\n",
      "    8.0 B     Total params\n",
      "    32,121.045Total estimated model params size (MB)\n",
      "[NeMo I 2025-07-04 06:36:49 nlp_adapter_mixins:208] After adding PEFT params:\n",
      "      | Name  | Type     | Params | Mode \n",
      "    -------------------------------------------\n",
      "    0 | model | GPTModel | 8.0 B  | train\n",
      "    -------------------------------------------\n",
      "    10.5 M    Trainable params\n",
      "    8.0 B     Non-trainable params\n",
      "    8.0 B     Total params\n",
      "    32,162.988Total estimated model params size (MB)\n",
      "[NeMo I 2025-07-04 06:36:49 megatron_gpt_generate:158] Freezing parameters for PEFT eval:\n",
      "      | Name  | Type     | Params | Mode\n",
      "    ------------------------------------------\n",
      "    0 | model | GPTModel | 8.0 B  | eval\n",
      "    ------------------------------------------\n",
      "    0         Trainable params\n",
      "    8.0 B     Non-trainable params\n",
      "    8.0 B     Total params\n",
      "    32,162.988Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-04 06:36:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:161: You have overridden `MegatronGPTSFTModel.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.\n",
      "    \n",
      "[NeMo W 2025-07-04 06:36:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:143: You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index. This will also not work well with gradient accumulation. This feature is very experimental and subject to change. Here be dragons.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-04 06:36:49 megatron_gpt_sft_model:803] Building GPT SFT test datasets.\n",
      "[NeMo I 2025-07-04 06:36:49 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2025-07-04 06:36:49 text_memmap_dataset:525] Processing 1 data files using 6 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-04 06:36:49 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.202712\n",
      "[NeMo I 2025-07-04 06:36:49 text_memmap_dataset:525] Processing 1 data files using 6 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-04 06:36:49 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.198920\n",
      "[NeMo I 2025-07-04 06:36:49 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2025-07-04 06:36:49 text_memmap_dataset:249] Loading ./lora_tutorial/data/test_small.jsonl\n",
      "[NeMo I 2025-07-04 06:36:49 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001874\n",
      "[NeMo I 2025-07-04 06:36:49 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo I 2025-07-04 06:36:49 megatron_gpt_sft_model:806] Length of test dataset: 2\n",
      "[NeMo I 2025-07-04 06:36:49 megatron_gpt_sft_model:829] Building dataloader with consumed samples: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo W 2025-07-04 06:36:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "    \n",
      "[NeMo W 2025-07-04 06:36:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:149: Found `dataloader_iter` argument in the `test_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n",
      "[NeMo W 2025-07-04 06:36:49 nemo_logging:349] /opt/apex/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest\n",
      "      warnings.warn(\"This function is only for unittest\")\n",
      "    \n",
      "[NeMo W 2025-07-04 06:36:51 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/modules/common/text_generation_utils.py:395: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "      input_info_tensor = torch.cuda.FloatTensor(input_info)\n",
      "    \n",
      "[NeMo W 2025-07-04 06:36:51 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/modules/common/text_generation_utils.py:403: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "      string_tensor = torch.as_tensor(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:11<00:00,  0.17it/s][NeMo I 2025-07-04 06:37:01 megatron_gpt_sft_model:561] Total deduplicated inference data size: 2 to 2\n",
      "[NeMo I 2025-07-04 06:37:01 megatron_gpt_sft_model:712] Predictions saved to customer_support_lora_test_customer_support_inputs_preds_labels.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-04 06:37:01 megatron_gpt_sft_model:652] No training data found, reconfiguring microbatches based on validation batch sizes.\n",
      "[NeMo W 2025-07-04 06:37:01 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2025-07-04 06:37:01 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('test_loss_customer_support', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2025-07-04 06:37:01 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('test_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:11<00:00,  0.17it/s]\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m       Test metric        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│\u001b[36m \u001b[0m\u001b[36m        test_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    2.4268767833709717    \u001b[0m\u001b[35m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36mtest_loss_customer_support\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    2.4268767833709717    \u001b[0m\u001b[35m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m         val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    2.4268767833709717    \u001b[0m\u001b[35m \u001b[0m│\n",
      "└────────────────────────────┴────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Run inference using the trained LoRA adapter\n",
    "MODEL=\"/root/verb-workspace/lora_tutorial/models/llama-3-8b-instruct-nemo_v1.0/8b_instruct_nemo_bf16.nemo\"\n",
    "TEST_DS=\"[./lora_tutorial/data/test_small.jsonl]\"\n",
    "TEST_NAMES=\"[customer_support]\"\n",
    "\n",
    "# Path to the LoRA checkpoint - use the actual file name\n",
    "LORA_CKPT=\"./lora_tutorial/experiments/customer_support_lora/checkpoints/customer_support_lora.nemo\"\n",
    "\n",
    "# Run generation\n",
    "python /root/verb-workspace/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py \\\n",
    "    model.restore_from_path=${MODEL} \\\n",
    "    model.peft.restore_from_path=${LORA_CKPT} \\\n",
    "    trainer.devices=1 \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    model.data.test_ds.file_names=${TEST_DS} \\\n",
    "    model.data.test_ds.names=${TEST_NAMES} \\\n",
    "    model.data.test_ds.global_batch_size=1 \\\n",
    "    model.data.test_ds.micro_batch_size=1 \\\n",
    "    model.data.test_ds.tokens_to_generate=100 \\\n",
    "    model.tensor_model_parallel_size=1 \\\n",
    "    model.pipeline_model_parallel_size=1 \\\n",
    "    inference.greedy=True \\\n",
    "    model.data.test_ds.output_file_path_prefix=customer_support_lora \\\n",
    "    model.data.test_ds.write_predictions_to_file=True \\\n",
    "    model.data.test_ds.add_bos=False \\\n",
    "    model.data.test_ds.add_eos=True \\\n",
    "    model.data.test_ds.add_sep=False \\\n",
    "    model.data.test_ds.label_key=\"output\" \\\n",
    "    model.data.test_ds.prompt_template=\"\\{input\\} \\{output\\}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "🎤 **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Let me explain what just happened in that output:\n",
    "\n",
    "**1. Tokenizer Warnings** (those repeated messages):\n",
    "These are harmless warnings from HuggingFace. What's happening:\n",
    "- NeMo uses multiprocessing to speed up data loading\n",
    "- Each process needs its own tokenizer instance\n",
    "- The warning is just saying 'Hey, I'm disabling parallel tokenization to avoid conflicts'\n",
    "\n",
    "You can silence these by setting: `export TOKENIZERS_PARALLELISM=false`\n",
    "\n",
    "**2. Data Processing**:\n",
    "- `Loading data files`: Reading your test JSONL file\n",
    "- `Length of test dataset: 2`: Found our 2 test examples\n",
    "- `Building dataloader`: Preparing batches for inference\n",
    "\n",
    "**3. The Inference Progress Bar**:\n",
    "`Testing DataLoader 0: 100%|██████████| 2/2`\n",
    "- Processed both test examples\n",
    "- Took about 11 seconds (0.17 items/second)\n",
    "- This is SLOW because we're generating 100 tokens per example\n",
    "\n",
    "**4. Results Saved**:\n",
    "`Predictions saved to customer_support_lora_test_customer_support_inputs_preds_labels.jsonl`\n",
    "- This file contains the model's actual responses!\n",
    "\n",
    "**5. Test Metrics Table**:\n",
    "- `test_loss: 2.427` - This is the perplexity loss on test data\n",
    "- Lower is better (1.0 would be perfect)\n",
    "- 2.4 is actually quite good for a small LoRA adapter!\n",
    "\n",
    "The test metrics table shows your LoRA model's **loss score** (lower is better), which measures how different the model's predictions are from your training examples. A score of **0-1 is excellent** (but may indicate memorization), **1-2.5 is good** (your 2.427 falls here!), **2.5-4 is okay**, and **4+ needs work**. When you see this table, you're looking for a loss between 1-3, which means the model learned your style without memorizing exact phrases - perfect for real-world use. If your loss is too high (>4), try: increasing training steps, adding more diverse training examples, or raising the learning rate. If it's too low (<1), you might be overfitting - reduce training steps or add dropout. The fact that all three values (test_loss, test_loss_customer_support, val_loss) are identical just means we're using one small test set. Your 2.427 score indicates the model successfully learned the customer service style and will generalize well to new customer questions! \n",
    "\n",
    "Here's why they're identical:\n",
    "- test_loss: The average loss across ALL test datasets\n",
    "- test_loss_customer_support: The loss for your specific \"customer_support\" test set\n",
    "- val_loss: Validation loss (but in inference mode, it uses test data)\n",
    "\n",
    "They're the same because:\n",
    "- You only have ONE test dataset (customer_support)\n",
    "- So the \"average of all datasets\" = \"customer_support dataset\" = same number\n",
    "- In inference/test mode, validation and test use the same data\n",
    "\n",
    "\n",
    "\n",
    "The key takeaway: Your LoRA adapter successfully loaded and generated responses!\n",
    "Now let's look at what it actually said...\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare baseline predictions with LoRA predictions\n",
    "print(\"=== BASELINE predictions (without LoRA): ===\")\n",
    "!head -n2 baseline_no_lora_test_baseline_inputs_preds_labels.jsonl\n",
    "\n",
    "print(\"\\n=== LoRA predictions (with fine-tuning): ===\")\n",
    "!head -n2 customer_support_lora_test_customer_support_inputs_preds_labels.jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"input\": \"User: My package is damaged. What should I do?\\n\\nAssistant:\", \"pred\": \" I'm sorry to hear you're experiencing issues with your package. Please take photos of the damage and packaging, then contact us with your order number. We'll arrange a replacement or refund immediately.\", \"label\": \" I'm sorry to hear you received a damaged product. Please take photos of the damage and packaging, then contact us with your order number. We'll arrange a replacement or refund immediately.\"}\n",
      "{\"input\": \"User: How do I track my order?\\n\\nAssistant:\", \"pred\": \" I'd be happy to help you track your order. Please provide your order number and we'll check the status for you immediately.\", \"label\": \" You can track your order by logging into your account and clicking 'Order History', or use the tracking link in your confirmation email. The tracking number will show real-time updates.\"}\n"
     ]
    }
   ],
   "source": [
    "# Look at the generated predictions\n",
    "!head -n2 customer_support_lora_test_customer_support_inputs_preds_labels.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export LoRA for Deployment [STOP]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎤 **PRESENTER SCRIPT:**\n",
    "\n",
    "\"For production deployment, we need to optimize our model. This script handles the export pipeline:\n",
    "\n",
    "[EXPLAIN EACH FUNCTION]\n",
    "\n",
    "**Step 1: Merge LoRA Weights**\n",
    "This combines base model + LoRA adapter into a single model. Mathematically:\n",
    "- W_new = W_original + A × B × scaling\n",
    "\n",
    "The merge is permanent but eliminates adapter overhead at inference.\n",
    "\n",
    "**Step 2: Export to TensorRT-LLM**\n",
    "This is NVIDIA's secret weapon - TensorRT optimization:\n",
    "- Kernel fusion: Combines operations\n",
    "- Quantization: INT8/FP8 precision\n",
    "- Graph optimization: Removes redundancy\n",
    "- Hardware specific: Optimizes for YOUR GPU\n",
    "\n",
    "Performance improvements:\n",
    "- 2-5x throughput increase\n",
    "- 50-70% latency reduction\n",
    "- 30-50% memory savings\n",
    "\n",
    "[SAVE THE SCRIPT]\n",
    "\n",
    "The complete pipeline:\n",
    "1. Train LoRA adapter (6 hours)\n",
    "2. Merge with base model (5 minutes)\n",
    "3. Export to TensorRT (20 minutes)\n",
    "4. Deploy as NIM (instant)\n",
    "\n",
    "You've gone from idea to optimized deployment in under a day!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Merge LoRA Weights (Optional)\n",
    "\n",
    "To merge the LoRA adapter with the base model for deployment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a script to merge LoRA weights\n",
    "merge_script = \"\"\"#!/bin/bash\n",
    "# Merge LoRA weights with base model\n",
    "\n",
    "python /root/verb-workspace/NeMo/scripts/nlp_language_modeling/merge_lora_weights/merge.py \\\\\n",
    "    --lora_checkpoint_path ./lora_tutorial/experiments/customer_support_lora/checkpoints/customer_support_lora.nemo \\\\\n",
    "    --base_checkpoint_path /root/verb-workspace/lora_tutorial/models/llama-3-8b-instruct-nemo_v1.0/8b_instruct_nemo_bf16.nemo \\\\\n",
    "    --output_path ./lora_tutorial/models/llama3-8b-customer-support-merged.nemo\n",
    "\"\"\"\n",
    "\n",
    "with open(\"lora_tutorial/merge_lora.sh\", \"w\") as f:\n",
    "    f.write(merge_script)\n",
    "\n",
    "!chmod +x lora_tutorial/merge_lora.sh\n",
    "print(\"Created merge script: lora_tutorial/merge_lora.sh\")\n",
    "print(\"\\nTo merge the weights, run:\")\n",
    "print(\"bash lora_tutorial/merge_lora.sh\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create export script template\n",
    "export_script = '''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Export LoRA model for deployment with NIMs\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "def merge_lora_weights(base_model_path, lora_checkpoint_path, output_path):\n",
    "    \"\"\"\n",
    "    Merge LoRA weights with base model\n",
    "    \"\"\"\n",
    "    merge_config = {\n",
    "        \"lora_checkpoint_path\": lora_checkpoint_path,\n",
    "        \"base_checkpoint_path\": base_model_path,\n",
    "        \"output_path\": output_path,\n",
    "        \"tensor_model_parallel_size\": 1,\n",
    "        \"pipeline_model_parallel_size\": 1,\n",
    "        \"gpus_per_node\": 1,\n",
    "        \"num_nodes\": 1,\n",
    "        \"precision\": \"bf16\"\n",
    "    }\n",
    "    \n",
    "    # Run merge script\n",
    "    cmd = f\"\"\"\n",
    "    python /root/verb-workspace/NeMo/scripts/nlp_language_modeling/merge_lora_weights/merge.py \\\\\\\\\n",
    "        --lora_checkpoint_path {lora_checkpoint_path} \\\\\\\\\n",
    "        --base_checkpoint_path {base_model_path} \\\\\\\\\n",
    "        --output_path {output_path}\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Merging LoRA weights...\")\n",
    "    print(f\"Command: {cmd}\")\n",
    "    \n",
    "def export_to_trt_llm(merged_model_path, output_dir):\n",
    "    \"\"\"\n",
    "    Export to TensorRT-LLM for NIM deployment\n",
    "    \"\"\"\n",
    "    cmd = f\"\"\"\n",
    "    python /root/verb-workspace/NeMo/scripts/export/export_to_trt_llm.py \\\\\\\\\n",
    "        --nemo_checkpoint {merged_model_path} \\\\\\\\\n",
    "        --model_type llama \\\\\\\\\n",
    "        --model_repository {output_dir} \\\\\\\\\n",
    "        --max_input_len 1024 \\\\\\\\\n",
    "        --max_output_len 1024 \\\\\\\\\n",
    "        --max_batch_size 8 \\\\\\\\\n",
    "        --dtype bfloat16\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Exporting to TensorRT-LLM...\")\n",
    "    print(f\"Command: {cmd}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    merge_lora_weights(\n",
    "        \"path/to/base/model.nemo\",\n",
    "        \"path/to/lora/checkpoint.nemo\",\n",
    "        \"path/to/merged/model.nemo\"\n",
    "    )\n",
    "    \n",
    "    export_to_trt_llm(\n",
    "        \"path/to/merged/model.nemo\",\n",
    "        \"./trt_models/custom\"\n",
    "    )\n",
    "'''\n",
    "\n",
    "# Save export script\n",
    "with open(\"lora_tutorial/export_lora.py\", \"w\") as f:\n",
    "    f.write(export_script)\n",
    "\n",
    "print(\"Created export script: lora_tutorial/export_lora.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Practices Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎤 **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Let me share hard-won best practices from training dozens of LoRA models:\n",
    "\n",
    "[RUN THE CELL TO CREATE THE GUIDE]\n",
    "\n",
    "**1. Dataset Preparation**\n",
    "The #1 failure mode is bad data. I've seen teams waste weeks because of:\n",
    "- Inconsistent formatting\n",
    "- Contradictory examples\n",
    "- Poor quality responses\n",
    "- Unbalanced categories\n",
    "\n",
    "Solution: Spend 80% of your time on data, 20% on training.\n",
    "\n",
    "**2. Hyperparameters**\n",
    "Start conservative:\n",
    "- Rank 16 (increase if underfitting)\n",
    "- Learning rate 1e-4 (increase if slow)\n",
    "- Batch size: as large as GPU allows\n",
    "- Epochs: 3-5 (watch validation loss!)\n",
    "\n",
    "**3. Target Modules**\n",
    "- Start with just attention_qkv\n",
    "- Add attention_dense if needed\n",
    "- MLP layers only for major behavior changes\n",
    "- More modules = slower training but more capacity\n",
    "\n",
    "**4. Monitoring**\n",
    "Watch these metrics:\n",
    "- Training loss: Should decrease smoothly\n",
    "- Validation loss: Should follow training loss\n",
    "- Gradient norms: Should stay stable\n",
    "- Learning rate: Verify schedule\n",
    "\n",
    "Red flags:\n",
    "- Validation loss increases (overfitting)\n",
    "- Loss spikes (bad examples)\n",
    "- NaN losses (learning rate too high)\n",
    "\n",
    "**5. Deployment**\n",
    "- Always test merged models\n",
    "- Keep original adapters for updates\n",
    "- Version control everything\n",
    "- A/B test in production\n",
    "\n",
    "Remember: LoRA is powerful but not magic. It modifies behavior, doesn't add knowledge. You can't teach it facts it never knew, but you can teach it how to use what it knows!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a best practices summary\n",
    "best_practices = \"\"\"\n",
    "# LoRA Fine-tuning Best Practices\n",
    "\n",
    "## 1. Dataset Preparation\n",
    "- Use high-quality, task-specific data\n",
    "- 1000-10000 examples often sufficient\n",
    "- Include diverse examples\n",
    "- Format: JSONL with 'input' and 'output' fields\n",
    "\n",
    "## 2. Hyperparameters\n",
    "- Rank (adapter_dim): Start with 16-32\n",
    "- Learning rate: 1e-4 to 5e-4\n",
    "- Batch size: As large as GPU memory allows\n",
    "- Epochs: 3-5 (watch for overfitting)\n",
    "\n",
    "## 3. Target Modules\n",
    "- attention_qkv: Most common choice\n",
    "- Can also target: attention_dense, mlp_fc1, mlp_fc2\n",
    "- More modules = more capacity but slower training\n",
    "\n",
    "## 4. Monitoring\n",
    "- Track validation loss\n",
    "- Test on held-out examples\n",
    "- Save checkpoints frequently\n",
    "- Use early stopping if needed\n",
    "\n",
    "## 5. Deployment\n",
    "- Merge weights for production\n",
    "- Export to TensorRT for optimization\n",
    "- Test thoroughly before deployment\n",
    "- Keep original adapter files for updates\n",
    "\"\"\"\n",
    "\n",
    "with open(\"lora_tutorial/best_practices.md\", \"w\") as f:\n",
    "    f.write(best_practices)\n",
    "\n",
    "print(\"Created best practices guide\")\n",
    "print(\"\\\\nAll tutorial files created in ./lora_tutorial/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎤 **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Let's see everything we've created in our LoRA tutorial workspace:\n",
    "\n",
    "[RUN THE CELL]\n",
    "\n",
    "Perfect! We have:\n",
    "- Training data ready\n",
    "- Configuration defined\n",
    "- Scripts for the complete pipeline\n",
    "- Best practices documented\n",
    "\n",
    "This is a professional setup ready for real model training. In production, you'd add:\n",
    "- Git version control\n",
    "- Experiment tracking (MLflow/W&B)\n",
    "- Automated testing\n",
    "- CI/CD pipelines\n",
    "- Model registry\n",
    "\n",
    "But this foundation is solid!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all created files\n",
    "import os\n",
    "for root, dirs, files in os.walk(\"lora_tutorial\"):\n",
    "    level = root.replace(\"lora_tutorial\", \"\").count(os.sep)\n",
    "    indent = \" \" * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = \" \" * 2 * (level + 1)\n",
    "    for file in files:\n",
    "        print(f\"{subindent}{file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎤 **PRESENTER SCRIPT:**\n",
    "\n",
    "\"Incredible work! You've mastered LoRA fine-tuning. Let's celebrate what you've learned:\n",
    "\n",
    "✅ **LoRA Theory**: Low-rank matrix decomposition for efficient adaptation\n",
    "✅ **Parameter Efficiency**: Train <1% of parameters for 95% of performance\n",
    "✅ **Data Preparation**: Quality > quantity, JSONL format\n",
    "✅ **Configuration**: Rank, target modules, hyperparameters\n",
    "✅ **Training Pipeline**: NeMo integration, distributed training\n",
    "✅ **Inference Options**: Dynamic adapters vs merged models\n",
    "✅ **Export & Optimization**: TensorRT for production performance\n",
    "✅ **Best Practices**: Data quality, monitoring, deployment strategies\n",
    "\n",
    "You can now:\n",
    "- Take any open-source LLM\n",
    "- Customize it for your specific needs\n",
    "- Do it on affordable hardware\n",
    "- Deploy it efficiently\n",
    "\n",
    "Real-world applications I've seen:\n",
    "- Legal firms: Contract analysis in their style\n",
    "- Healthcare: Medical report generation\n",
    "- Finance: Compliance-aware responses\n",
    "- Retail: Product description generation\n",
    "- Gaming: NPC dialogue systems\n",
    "\n",
    "But here's the final challenge: How do we deploy these custom models at scale? How do we serve multiple LoRA adapters efficiently? How do we ensure production reliability?\n",
    "\n",
    "That's our grand finale - Part 4: Deploying LoRA models with NIMs. We'll build a production system that can serve your custom models to millions of users.\n",
    "\n",
    "Ready to complete your journey from prototype to production? Let's go!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
