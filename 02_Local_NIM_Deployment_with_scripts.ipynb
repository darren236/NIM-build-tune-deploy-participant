{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 2: Local NIM Deployment\n",
        "\n",
        "This notebook will guide you through deploying NVIDIA NIMs locally on your own infrastructure.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- NVIDIA GPU (compute capability ‚â• 7.0)\n",
        "- Docker installed\n",
        "- NVIDIA Container Toolkit\n",
        "- NGC API Key\n",
        "- Sufficient disk space (~50GB per model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ensure GPU Availability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This cell is performing a GPU availability test to verify that Docker can properly access the NVIDIA GPUs on the system. \n",
        "\n",
        "This command tests if Docker can access the NVIDIA GPUs by running nvidia-smi inside a temporary Ubuntu container with GPU support enabled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify Docker and NVIDIA runtime\n",
        "!docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Time to set up NGC (NVIDIA GPU Cloud) access. This is different from the API key we used earlier - this one lets us download the actual NIM containers.\n",
        "\n",
        "We're also creating a cache directory. This is important - models are LARGE (5-100GB). The cache means:\n",
        "- Download once, use many times\n",
        "- Survive container restarts\n",
        "- Share models between containers\n",
        "- Quick model switching\n",
        "\n",
        "The cache will be at ~/.cache/nim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# Set up environment variables\n",
        "NGC_API_KEY = os.getenv('NGC_API_KEY')\n",
        "if not NGC_API_KEY:\n",
        "    import getpass\n",
        "    NGC_API_KEY = getpass.getpass(\"Enter your NGC API key: \")\n",
        "    os.environ['NGC_API_KEY'] = NGC_API_KEY\n",
        "\n",
        "# Set cache directory\n",
        "LOCAL_NIM_CACHE = os.path.expanduser(\"~/.cache/nim\")\n",
        "os.makedirs(LOCAL_NIM_CACHE, exist_ok=True)\n",
        "print(f\"NIM cache directory: {LOCAL_NIM_CACHE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get NVIDIA API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "from pathlib import Path\n",
        "\n",
        "# Find the .env file in the project root\n",
        "env_path = Path('.env')\n",
        "\n",
        "# Load environment variables from .env file\n",
        "# Use override=True to ensure values are loaded even if they exist in environment\n",
        "load_dotenv(dotenv_path=env_path, override=True)\n",
        "\n",
        "# Get API key from environment\n",
        "nvidia_api_key = os.getenv(\"NVIDIA_API_KEY\")\n",
        "\n",
        "if not nvidia_api_key:\n",
        "    print(\"‚ùå NVIDIA API Key not found in .env file!\")\n",
        "    print(\"üëâ Please run 00_Workshop_Setup.ipynb first to set up your API key.\")\n",
        "    print(f\"   (Looked for .env file at: {env_path.absolute()})\")\n",
        "    raise ValueError(\"NVIDIA_API_KEY not found. Please run the setup notebook first.\")\n",
        "else:\n",
        "    print(\"‚úÖ NVIDIA API Key loaded successfully from .env file\")\n",
        "    os.environ[\"NVIDIA_API_KEY\"] = nvidia_api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. NGC Authentication"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Log into NGC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Docker login to NGC\n",
        "login_cmd = f'echo \"{NGC_API_KEY}\" | docker login nvcr.io --username \\'$oauthtoken\\' --password-stdin'\n",
        "result = subprocess.run(login_cmd, shell=True, capture_output=True, text=True)\n",
        "print(\"Login result:\", result.stdout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Deploy Your First NIM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The startup process:\n",
        "1. Pull NIM image (done in setup notebook)\n",
        "2. Check cache for model files\n",
        "3. Load model into GPU memory (May take awhile)\n",
        "4. Start inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define deployment parameters\n",
        "CONTAINER_NAME = \"llama3.1-8b-instruct\" \n",
        "IMG_NAME = \"nvcr.io/nim/meta/llama-3.1-8b-instruct:latest\"\n",
        "\n",
        "# Stop existing container if running\n",
        "!docker stop {CONTAINER_NAME} 2>/dev/null || true\n",
        "!docker rm {CONTAINER_NAME} 2>/dev/null || true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "This cell deploys the NIM container with the Llama 3.1 8B Instruct model. It constructs and executes a Docker command that:\n",
        "\n",
        "**Container Configuration:**\n",
        "- Runs in detached mode (`-d`) for background operation\n",
        "- Enables GPU access with NVIDIA runtime\n",
        "- Allocates 16GB shared memory for PyTorch operations\n",
        "- Mounts the local cache directory to persist downloaded models\n",
        "- Maps port 8000 for API access\n",
        "- Runs with the current user's permissions to avoid file permission issues\n",
        "\n",
        "**Key Environment Variables:**\n",
        "- `NGC_API_KEY`: Authenticates with NVIDIA GPU Cloud to download the model\n",
        "- `LOCAL_NIM_CACHE`: Points to `~/.cache/nim` for model storage\n",
        "\n",
        "**Success/Failure Handling:**\n",
        "- On success: Displays the container ID and confirms deployment\n",
        "- On failure: Provides detailed troubleshooting steps including:\n",
        "  - Docker status checks\n",
        "  - Port conflict detection\n",
        "  - GPU availability verification\n",
        "  - Disk space validation\n",
        "  - NGC authentication verification\n",
        "\n",
        "The first run will download the model (5-10 minutes), while subsequent runs use the cached model for faster startup (30-60 seconds)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start NIM container\n",
        "docker_cmd = f\"\"\"\n",
        "docker run -d --name={CONTAINER_NAME} \\\n",
        "    --runtime=nvidia \\\n",
        "    --gpus all \\\n",
        "    --shm-size=16GB \\\n",
        "    -e NGC_API_KEY={NGC_API_KEY} \\\n",
        "    -v {LOCAL_NIM_CACHE}:/opt/nim/.cache \\\n",
        "    -u $(id -u) \\\n",
        "    -p 8000:8000 \\\n",
        "    {IMG_NAME}\n",
        "\"\"\"\n",
        "\n",
        "print(\"Starting NIM container...\")\n",
        "result = subprocess.run(docker_cmd, shell=True, capture_output=True, text=True)\n",
        "\n",
        "# Check if the command succeeded\n",
        "if result.returncode == 0 and result.stdout.strip():\n",
        "    container_id = result.stdout.strip()\n",
        "    print(f\"‚úÖ Container started successfully!\")\n",
        "    print(f\"Container ID: {container_id}\")\n",
        "else:\n",
        "    print(\"‚ùå Failed to start container!\")\n",
        "    print(f\"Return code: {result.returncode}\")\n",
        "    if result.stderr:\n",
        "        print(f\"Error message: {result.stderr}\")\n",
        "    if result.stdout:\n",
        "        print(f\"Output: {result.stdout}\")\n",
        "    \n",
        "    # Common issues and solutions\n",
        "    print(\"\\nTroubleshooting tips:\")\n",
        "    print(\"1. Check if Docker is running: docker info\")\n",
        "    print(\"2. Check if image exists: docker images | grep llama\")\n",
        "    print(\"3. Check if port 8000 is already in use: docker ps -a\")\n",
        "    print(f\"4. Check Docker logs: docker logs {CONTAINER_NAME}\")\n",
        "    print(\"5. Verify NGC authentication: echo $NGC_API_KEY\")\n",
        "    print(\"6. Check available disk space: df -h\")\n",
        "    print(\"7. Verify GPU is accessible: nvidia-smi\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This function polls the health endpoint until the NIM is ready.\n",
        "\n",
        "[When ready appears]\n",
        "\n",
        "The NIM is now ready to serve requests through the familiar OpenAI API format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def wait_for_nim_ready(max_attempts=60, sleep_time=5):\n",
        "    \"\"\"Wait for NIM to be ready to serve requests\"\"\"\n",
        "    print(\"Waiting for NIM to start (this may take a few minutes on first run)...\")\n",
        "    \n",
        "    # Get container IP\n",
        "    import subprocess\n",
        "    import json\n",
        "    \n",
        "    try:\n",
        "        result = subprocess.run(['docker', 'inspect', CONTAINER_NAME], \n",
        "                              capture_output=True, text=True)\n",
        "        container_info = json.loads(result.stdout)\n",
        "        container_ip = container_info[0]['NetworkSettings']['IPAddress']\n",
        "        health_url = f\"http://{container_ip}:8000/v1/health/ready\"\n",
        "    except:\n",
        "        health_url = \"http://localhost:8000/v1/health/ready\"  # fallback\n",
        "    \n",
        "    for attempt in range(max_attempts):\n",
        "        try:\n",
        "            response = requests.get(health_url)\n",
        "            if response.status_code == 200:\n",
        "                print(\"\\n‚úÖ NIM is ready!\")\n",
        "                return True\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        print(\".\", end=\"\", flush=True)\n",
        "        time.sleep(sleep_time)\n",
        "    \n",
        "    print(\"\\n‚ùå NIM failed to start\")\n",
        "    return False\n",
        "\n",
        "# Wait for container to be ready\n",
        "if wait_for_nim_ready():\n",
        "    print(\"NIM is ready to serve requests!\")\n",
        "else:\n",
        "    print(\"Check logs with: docker logs\", CONTAINER_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This cell retrieves the container's internal IP address and tests the NIM API directly using that IP instead of localhost:8000.\n",
        "\n",
        "**Why this is needed:**\n",
        "Since this workshop runs on cloud GPU instances (like Brev), localhost connections often fail because:\n",
        "- The cloud instance's network configuration may block local port forwarding\n",
        "- Security policies in cloud environments can restrict localhost access\n",
        "- Docker's bridge network might not properly route to the host's localhost\n",
        "\n",
        "**The workaround:**\n",
        "By using `docker inspect` to get the container's IP address (like 172.17.0.3), we bypass these cloud networking issues and connect directly to the container's network. This ensures reliable API access regardless of the cloud provider's network configuration.\n",
        "\n",
        "The cell then verifies the NIM is working by requesting the available models list, confirming the API is ready to serve requests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import json\n",
        "\n",
        "# Get container IP address\n",
        "def get_container_ip(container_name):\n",
        "    try:\n",
        "        result = subprocess.run(['docker', 'inspect', container_name], \n",
        "                              capture_output=True, text=True)\n",
        "        if result.returncode == 0:\n",
        "            container_info = json.loads(result.stdout)\n",
        "            ip = container_info[0]['NetworkSettings']['IPAddress']\n",
        "            print(f\"Container IP: {ip}\")\n",
        "            return ip\n",
        "        else:\n",
        "            print(f\"Failed to get container info for '{container_name}'\")\n",
        "            print(f\"Error: {result.stderr}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting container IP: {e}\")\n",
        "        return None\n",
        "\n",
        "container_ip = get_container_ip(CONTAINER_NAME)\n",
        "\n",
        "# If we have the IP, try connecting to it directly\n",
        "if container_ip:\n",
        "    try:\n",
        "        response = requests.get(f\"http://{container_ip}:8000/v1/models\", timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            print(\"‚úÖ NIM is accessible via container IP!\")\n",
        "            print(\"Available models:\", response.json())\n",
        "        else:\n",
        "            print(f\"‚ùå Got status code {response.status_code} from container IP\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error connecting to container IP: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Test Local NIM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This cell shows how to use the OpenAI Python SDK with your local NIM - the same tool we used with the cloud API in Part 1.\n",
        "\n",
        "**What's happening:**\n",
        "1. Creates an OpenAI client, but points it to your local container instead of the cloud\n",
        "2. No API key needed since it's running locally\n",
        "3. Requests a poem about AI with streaming enabled\n",
        "4. Prints the response word-by-word as it's generated\n",
        "\n",
        "**The key insight:** You can switch between cloud and local NIMs by just changing the `base_url`. All your existing OpenAI code works unchanged. The commented section shows how easy it is to switch back to the cloud API - just swap the client configuration.\n",
        "\n",
        "This demonstrates the power of NIMs: same API, same code, but now running on your own hardware."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import subprocess\n",
        "import json\n",
        "\n",
        "# Create OpenAI client pointing to your local NIM\n",
        "client = OpenAI(\n",
        "    base_url=f\"http://{container_ip}:8000/v1\",\n",
        "    api_key=\"not-needed-for-local\",  # Local NIM doesn't require autu\n",
        ")\n",
        "\n",
        "# You can reference how we called this model via the API\n",
        "# client = OpenAI(\n",
        "#     base_url=\"https://integrate.api.nvidia.com/v1\",\n",
        "#     api_key=nvidia_api_key\n",
        "# )\n",
        "\n",
        "# Example: Streaming response\n",
        "stream = client.chat.completions.create(\n",
        "    model=\"meta/llama-3.1-8b-instruct\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Write a short poem about AI\"}\n",
        "    ],\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "print(\"Streaming response:\")\n",
        "for chunk in stream:\n",
        "    if chunk.choices[0].delta.content:\n",
        "        print(chunk.choices[0].delta.content, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note on output: Your poem will be different from the example shown because:\n",
        "- LLMs generate unique responses each time (temperature=0.7 adds randomness)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Models availale in local NIM deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This cell checks which models are available in your local NIM deployment.\n",
        "\n",
        "**What it does:**\n",
        "- Sends a request to the `/v1/models` endpoint\n",
        "- Retrieves metadata about the deployed model\n",
        "- Pretty-prints the information in readable JSON format\n",
        "\n",
        "**What you'll see:**\n",
        "- Model ID and version\n",
        "- Creation timestamp\n",
        "- Available permissions and settings\n",
        "- Maximum context length (tokens the model can handle)\n",
        "\n",
        "This is useful for confirming which model is actually running in your container and verifying the deployment was successful.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check available models\n",
        "response = requests.get(f\"http://{container_ip}:8000/v1/models\")\n",
        "models = response.json()\n",
        "print(\"Available models:\")\n",
        "print(json.dumps(models, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Clean Up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we move on to LoRA fine-tuning, let's properly clean up our deployment. This is important for a few reasons:\n",
        "\n",
        "1. Frees up GPU memory for our next activities\n",
        "2. Prevents port conflicts if we redeploy\n",
        "3. Good practice for resource management\n",
        "\n",
        "Don't worry - the model remains cached, so if you want to restart this NIM later, it'll start up in seconds, not minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop and remove container\n",
        "!docker stop {CONTAINER_NAME}\n",
        "!docker rm {CONTAINER_NAME}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "You've learned how to:\n",
        "- Deploy NIMs locally with Docker\n",
        "- Test deployments\n",
        "\n",
        "Next: Let's explore LoRA fine-tuning with NeMo!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
