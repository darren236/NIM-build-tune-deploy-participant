{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 4: Deploying LoRA Adapters with NVIDIA NIM\n",
        "\n",
        "This notebook demonstrates how to deploy your trained LoRA adapters using NVIDIA NIM. We'll cover:\n",
        "- Understanding NIM's LoRA deployment architecture\n",
        "- Using Docker volumes for reliable LoRA mounting\n",
        "- Testing your deployed LoRA adapter\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Before starting, ensure you have:\n",
        "1. Completed notebook 03 (LoRA training) - you should have a `.nemo` file\n",
        "2. Docker installed with GPU support\n",
        "3. Your NGC API key ready\n",
        "4. At least 20GB of free disk space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding NIM LoRA Deployment\n",
        "\n",
        "### How NIM Handles LoRA Adapters\n",
        "\n",
        "NVIDIA NIM supports dynamic LoRA loading through:\n",
        "- **NIM_PEFT_SOURCE**: Environment variable pointing to your LoRA directory\n",
        "- **Automatic Discovery**: NIM scans for `.nemo` files in subdirectories\n",
        "- **Hot Reloading**: With `NIM_PEFT_REFRESH_INTERVAL`, NIM checks for new adapters\n",
        "\n",
        "### Expected Directory Structure\n",
        "\n",
        "```\n",
        "NIM_PEFT_SOURCE/\n",
        "├── adapter1/\n",
        "│   └── adapter1.nemo\n",
        "├── adapter2/\n",
        "│   └── adapter2.nemo\n",
        "└── adapter3/\n",
        "    └── adapter3.nemo\n",
        "```\n",
        "\n",
        "Each adapter must be in its own subdirectory!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Important: Cloud GPU Deployment Challenges\n",
        "\n",
        "### The Bind Mount Problem\n",
        "\n",
        "On cloud GPU instances (AWS, GCP, Azure), Docker bind mounts often fail due to:\n",
        "- Storage driver incompatibilities\n",
        "- Security policies\n",
        "- Network file systems\n",
        "\n",
        "**Symptoms:**\n",
        "- Mounted directories appear empty inside containers\n",
        "- Files exist on host but not visible in container\n",
        "- No error messages, just empty directories\n",
        "\n",
        "### The Solution: Docker Volumes\n",
        "\n",
        "Docker named volumes work reliably where bind mounts fail. We'll use this approach throughout the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load NGC Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and imports\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Load environment variables from .env file\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "except ImportError:\n",
        "    # If python-dotenv is not installed, try to read .env manually\n",
        "    if os.path.exists('.env'):\n",
        "        with open('.env', 'r') as f:\n",
        "            for line in f:\n",
        "                if '=' in line:\n",
        "                    key, value = line.strip().split('=', 1)\n",
        "                    os.environ[key] = value\n",
        "\n",
        "# Set up environment\n",
        "NGC_API_KEY = os.getenv('NGC_API_KEY')\n",
        "if not NGC_API_KEY:\n",
        "    print(\"⚠️  NGC_API_KEY not found in environment or .env file!\")\n",
        "    print(\"Please run the Workshop Setup notebook (00_Workshop_Setup.ipynb) first.\")\n",
        "else:\n",
        "    os.environ['NGC_API_KEY'] = NGC_API_KEY\n",
        "\n",
        "print(f\"NGC API Key configured: {'✓' if NGC_API_KEY else '✗'}\")\n",
        "print(f\"Working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Log into NGC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Docker login to NGC\n",
        "login_cmd = f'echo \"{NGC_API_KEY}\" | docker login nvcr.io --username \\'$oauthtoken\\' --password-stdin'\n",
        "result = subprocess.run(login_cmd, shell=True, capture_output=True, text=True)\n",
        "\n",
        "if \"Login Succeeded\" in result.stdout:\n",
        "    print(\"✓ Successfully logged in to NGC\")\n",
        "else:\n",
        "    print(\"✗ Login failed!\")\n",
        "    print(\"Error:\", result.stderr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Prepare Your LoRA Adapter\n",
        "\n",
        "First, let's check that your LoRA adapter is ready for deployment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's make sure your LoRA adapter is ready. We're looking for the .nemo file from notebook 03."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for LoRA files\n",
        "lora_paths = [\n",
        "    \"lora_tutorial/experiments/customer_support_lora/checkpoints/customer_support_lora.nemo\",\n",
        "    \"loras/customer_support_lora/customer_support_lora.nemo\"\n",
        "]\n",
        "\n",
        "lora_file = None\n",
        "for path in lora_paths:\n",
        "    if os.path.exists(path):\n",
        "        lora_file = path\n",
        "        print(f\"✓ Found LoRA adapter: {path}\")\n",
        "        print(f\"  Size: {os.path.getsize(path) / 1024 / 1024:.2f} MB\")\n",
        "        break\n",
        "\n",
        "if not lora_file:\n",
        "    print(\"✗ No LoRA adapter found!\")\n",
        "    print(\"\\nPlease ensure you've completed notebook 03 and have a .nemo file.\")\n",
        "    print(\"Expected locations:\")\n",
        "    for path in lora_paths:\n",
        "        print(f\"  - {path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This cell prepares the LoRA adapter for deployment by creating the required directory structure (`loras/customer_support_lora`) and copying the trained LoRA file into it, which NIM expects for loading custom adapters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create proper directory structure for NIM\n",
        "!mkdir -p loras/customer_support_lora\n",
        "\n",
        "# Copy LoRA file if needed\n",
        "if lora_file and not os.path.exists(\"loras/customer_support_lora/customer_support_lora.nemo\"):\n",
        "    !cp {lora_file} loras/customer_support_lora/\n",
        "    print(\"✓ Copied LoRA adapter to deployment directory\")\n",
        "\n",
        "# Verify structure\n",
        "!echo \"LoRA deployment structure:\"\n",
        "!tree loras/ 2>/dev/null || find loras/ -type f -name \"*.nemo\" | head -10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Clean Up Existing Resources\n",
        "\n",
        "Before deploying, let's ensure we have a clean slate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "CONTAINER_NAME = \"llama3.1-lora-nim-volume\"\n",
        "VOLUME_NAME = \"nim-lora-adapters\"\n",
        "IMAGE_NAME = \"nvcr.io/nim/meta/llama-3.1-8b-instruct:latest\"\n",
        "\n",
        "# Clean up any existing resources\n",
        "print(\"🧹 Cleaning up existing resources...\")\n",
        "!docker rm -f {CONTAINER_NAME} 2>/dev/null || true\n",
        "!docker volume rm {VOLUME_NAME} 2>/dev/null || true\n",
        "\n",
        "print(\"\\n✓ Cleanup complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create Docker Volume and Copy LoRA Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This cell creates a Docker volume and copies the LoRA adapter file into it using two methods:\n",
        "\n",
        "1. **First attempt**: Uses a bind mount to copy files directly\n",
        "2. **Fallback method**: If that fails (common on cloud), uses `docker cp` with a temporary container\n",
        "\n",
        "The volume is needed because cloud GPU instances often have issues with direct file mounting, so Docker volumes provide a reliable way to make the LoRA file available to the NIM container."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Docker volume\n",
        "print(\"📦 Creating Docker volume for LoRA adapters...\")\n",
        "!docker volume create {VOLUME_NAME}\n",
        "\n",
        "# Copy LoRA files to the volume using a temporary container\n",
        "print(\"\\n📋 Copying LoRA files to Docker volume...\")\n",
        "# Method 1: Try with bind mount first\n",
        "copy_result = subprocess.run(\n",
        "    f'docker run --rm -v {VOLUME_NAME}:/data -v $(pwd)/loras:/source alpine sh -c '\n",
        "    f'\"mkdir -p /data/customer_support_lora && '\n",
        "    f'cp -r /source/customer_support_lora/* /data/customer_support_lora/ 2>/dev/null || echo \\'No files to copy\\' && '\n",
        "    f'ls -la /data/customer_support_lora/\"',\n",
        "    shell=True, capture_output=True, text=True\n",
        ")\n",
        "print(copy_result.stdout)\n",
        "\n",
        "# Method 2: Use docker cp as fallback (more reliable on cloud)\n",
        "print(\"\\n📋 Ensuring LoRA files are in the volume (using docker cp)...\")\n",
        "!docker run -d --name temp-container -v {VOLUME_NAME}:/data alpine sleep 3600\n",
        "!docker cp loras/customer_support_lora/customer_support_lora.nemo temp-container:/data/customer_support_lora/\n",
        "!docker exec temp-container ls -la /data/customer_support_lora/\n",
        "!docker rm -f temp-container\n",
        "\n",
        "print(\"\\n✓ LoRA files copied to volume\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This output shows the two-step copying process:\n",
        "\n",
        "**First attempt failed**: \"No files to copy\" - the bind mount method didn't work (common on cloud GPUs)\n",
        "\n",
        "**Second attempt succeeded**: \n",
        "- \"Successfully copied 21MB\" - the `docker cp` method worked\n",
        "- The LoRA file (`customer_support_lora.nemo`, 21MB) is now in the Docker volume\n",
        "- Ready for NIM to use\n",
        "\n",
        "This is why the cell uses two methods - the first one often fails on cloud, but the second one (docker cp) is more reliable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Start NIM Container with LoRA Support"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This cell starts the NIM container with LoRA support enabled.\n",
        "\n",
        "**Key configuration:**\n",
        "- `NIM_PEFT_SOURCE=/lora-store` - tells NIM where to find LoRA adapters\n",
        "- `NIM_PEFT_REFRESH_INTERVAL=300` - checks for new LoRAs every 5 minutes\n",
        "- `-v {VOLUME_NAME}:/lora-store` - mounts the Docker volume containing your LoRA file\n",
        "\n",
        "The container runs in the background with GPU access and will take 2-3 minutes to initialize on first run as it loads the base model and discovers the LoRA adapter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start NIM container with LoRA support\n",
        "docker_cmd = f\"\"\"\n",
        "docker run -d \\\\\n",
        "    --name={CONTAINER_NAME} \\\\\n",
        "    --runtime=nvidia \\\\\n",
        "    --gpus all \\\\\n",
        "    --shm-size=16GB \\\\\n",
        "    -e NGC_API_KEY={NGC_API_KEY} \\\\\n",
        "    -e NIM_PEFT_SOURCE=/lora-store \\\\\n",
        "    -e NIM_PEFT_REFRESH_INTERVAL=300 \\\\\n",
        "    -v {VOLUME_NAME}:/lora-store \\\\\n",
        "    -p 8000:8000 \\\\\n",
        "    {IMAGE_NAME}\n",
        "\"\"\"\n",
        "\n",
        "print(\"🚀 Starting NIM container with LoRA support...\")\n",
        "print(f\"\\nCommand: {docker_cmd}\")\n",
        "\n",
        "result = subprocess.run(docker_cmd, shell=True, capture_output=True, text=True)\n",
        "if result.returncode == 0:\n",
        "    container_id = result.stdout.strip()\n",
        "    print(f\"\\n✓ Container started: {container_id[:12]}\")\n",
        "    print(\"\\n⏳ Container is initializing. This may take 2-3 minutes on first run...\")\n",
        "else:\n",
        "    print(\"\\n✗ Failed to start container\")\n",
        "    print(\"Error:\", result.stderr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Get Container Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's find our container's IP address. On cloud instances, 'localhost' might not work, so we get the actual container IP.\n",
        "\n",
        "You'll see something like 172.17.0.3 - that's Docker's internal network.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get container IP address\n",
        "def get_container_ip():\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            f\"docker inspect -f '{{{{range.NetworkSettings.Networks}}}}{{{{.IPAddress}}}}{{{{end}}}}' {CONTAINER_NAME}\",\n",
        "            shell=True, capture_output=True, text=True\n",
        "        )\n",
        "        ip = result.stdout.strip()\n",
        "        return ip if ip else \"localhost\"\n",
        "    except:\n",
        "        return \"localhost\"\n",
        "\n",
        "container_ip = get_container_ip()\n",
        "print(f\"📍 Container IP: {container_ip}\")\n",
        "base_url = f\"http://{container_ip}:8000\"\n",
        "\n",
        "# Verify LoRA files are visible inside container\n",
        "print(\"\\n🔍 Verifying LoRA files in container...\")\n",
        "!docker exec {CONTAINER_NAME} ls -la /lora-store/customer_support_lora/ || echo \"Container still starting...\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Wait for NIM to Initialize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NIM needs time to initialize:\n",
        "- Loading the base model\n",
        "- Scanning for LoRA adapters\n",
        "- Optimizing for your GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wait for NIM to be ready\n",
        "def wait_for_nim(base_url, timeout=300):\n",
        "    print(\"⏳ Waiting for NIM to initialize...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    while time.time() - start_time < timeout:\n",
        "        try:\n",
        "            response = requests.get(f\"{base_url}/v1/health/ready\", timeout=2)\n",
        "            if response.status_code == 200:\n",
        "                print(\"\\n✅ NIM is ready!\")\n",
        "                return True\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        print(\".\", end=\"\", flush=True)\n",
        "        time.sleep(5)\n",
        "    \n",
        "    print(\"\\n✗ Timeout waiting for NIM\")\n",
        "    return False\n",
        "\n",
        "if wait_for_nim(base_url):\n",
        "    # Check logs for LoRA loading\n",
        "    print(\"\\n📋 Checking LoRA synchronization logs...\")\n",
        "    !docker logs {CONTAINER_NAME} 2>&1 | grep -i \"lora\\|peft\\|adapter\\|synchroniz\" | tail -20\n",
        "else:\n",
        "    print(\"\\n⚠️  NIM is taking longer than expected. Checking logs...\")\n",
        "    !docker logs {CONTAINER_NAME} 2>&1 | tail -30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Verify Available Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The moment of truth! Let's see what models are available.\n",
        "\n",
        "[RUN THE CELL]\n",
        "\n",
        "You should see TWO models:\n",
        "1. meta/llama3-8b-instruct - the base model\n",
        "2. customer_support_lora - your fine-tuned adapter\n",
        "\n",
        "If you only see the base model, give it another minute and run again. NIM might still be scanning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check available models\n",
        "try:\n",
        "    response = requests.get(f\"{base_url}/v1/models\")\n",
        "    if response.status_code == 200:\n",
        "        models = response.json()\n",
        "        print(\"📋 Available models:\")\n",
        "        print(\"=\" * 50)\n",
        "        \n",
        "        model_count = len(models.get('data', []))\n",
        "        print(f\"\\nFound {model_count} model(s):\\n\")\n",
        "        \n",
        "        for model in models.get('data', []):\n",
        "            model_id = model.get('id', 'unknown')\n",
        "            print(f\"  • {model_id}\")\n",
        "            if model_id == \"meta/llama3-8b-instruct\":\n",
        "                print(\"    Type: Base model\")\n",
        "            elif \"lora\" in model_id.lower():\n",
        "                print(\"    Type: LoRA adapter\")\n",
        "                print(\"    ✨ Your custom model is ready!\")\n",
        "        \n",
        "        if model_count == 1:\n",
        "            print(\"\\n⚠️  Only base model found. LoRA may still be loading...\")\n",
        "            print(\"Wait 30 seconds and run this cell again.\")\n",
        "        elif model_count > 1:\n",
        "            print(\"\\n✅ Both base model and LoRA adapter are available!\")\n",
        "    else:\n",
        "        print(f\"Error: Status code {response.status_code}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error connecting to NIM: {e}\")\n",
        "    print(\"\\nMake sure the container is running and healthy.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Test Your LoRA Adapter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now for the exciting part - let's test both models with the same query!\n",
        "\n",
        "Watch the difference:\n",
        "- Base model: Generic, helpful but not specific\n",
        "- LoRA model: Uses your training data, knows your policies\n",
        "\n",
        "This is the power of fine-tuning - domain-specific responses without training a whole new model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test function\n",
        "def test_model(model_name, query):\n",
        "    \"\"\"Test a model with a query\"\"\"\n",
        "    url = f\"{base_url}/v1/chat/completions\"\n",
        "    \n",
        "    data = {\n",
        "        \"model\": model_name,\n",
        "        \"messages\": [\n",
        "            # {\n",
        "            #     \"role\": \"system\",\n",
        "            #     \"content\": \"You are a helpful customer support assistant.\"\n",
        "            # },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": query\n",
        "            }\n",
        "        ],\n",
        "        \"max_tokens\": 150,\n",
        "        \"temperature\": 0.7\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        response = requests.post(url, json=data, timeout=30)\n",
        "        if response.status_code == 200:\n",
        "            return response.json()['choices'][0]['message']['content']\n",
        "        else:\n",
        "            return f\"Error: {response.status_code} - {response.text}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "# Test query\n",
        "test_query = \"My order hasn't arrived yet. Order number is 12345.\"\n",
        "\n",
        "print(\"🧪 Test Query:\", test_query)\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Test base model\n",
        "print(\"\\n🤖 BASE MODEL RESPONSE:\")\n",
        "print(\"-\" * 70)\n",
        "base_response = test_model(\"meta/llama-3.1-8b-instruct\", test_query)\n",
        "print(base_response)\n",
        "\n",
        "# Test LoRA model\n",
        "print(\"\\n🎯 LORA MODEL RESPONSE:\")\n",
        "print(\"-\" * 70)\n",
        "lora_response = test_model(\"customer_support_lora\", test_query)\n",
        "print(lora_response)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"💡 Notice the difference? The LoRA model provides more specific,\")\n",
        "print(\"   policy-aware responses based on your training data!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Test Multiple Scenarios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's test a few more scenarios, though not all would work because we had so little training data and training steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test multiple scenarios\n",
        "test_scenarios = [\n",
        "    \"How do I reset my password?\",\n",
        "    \"What is your return policy?\",\n",
        "    \"I received a damaged product. What should I do?\",\n",
        "    \"Do you offer international shipping?\"\n",
        "]\n",
        "\n",
        "print(\"🧪 Testing Multiple Scenarios\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, scenario in enumerate(test_scenarios, 1):\n",
        "    print(f\"\\n📌 Scenario {i}: {scenario}\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    # Test both models\n",
        "    base_response = test_model(\"meta/llama-3.1-8b-instruct\", scenario)\n",
        "    lora_response = test_model(\"customer_support_lora\", scenario)\n",
        "    \n",
        "    print(\"\\nBase Model:\")\n",
        "    print(base_response[:200] + \"...\" if len(base_response) > 200 else base_response)\n",
        "    \n",
        "    print(\"\\nLoRA Model:\")\n",
        "    print(lora_response[:200] + \"...\" if len(lora_response) > 200 else lora_response)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"✅ Testing complete! Your LoRA adapter is working perfectly!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "You've successfully deployed a LoRA adapter with NVIDIA NIM! Key takeaways:\n",
        "\n",
        "✅ **Use Docker volumes** for reliable deployment on cloud GPUs  \n",
        "✅ **Follow the directory structure** - each LoRA in its own subdirectory  \n",
        "✅ **NIM handles the complexity** - automatic discovery, optimized serving  \n",
        "\n",
        "### Resources\n",
        "\n",
        "- [NVIDIA NIM Documentation](https://docs.nvidia.com/nim/)\n",
        "- [NeMo Framework](https://github.com/NVIDIA/NeMo)\n",
        "- [NGC Catalog](https://catalog.ngc.nvidia.com)\n",
        "\n",
        "Happy NIM-ing! 🚀"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
