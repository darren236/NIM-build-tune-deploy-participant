{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 1: NVIDIA NIM API Tutorial\n",
        "\n",
        "In this tutorial, we'll learn how to use NVIDIA's NIM API for quick and easy access to optimized AI models.\n",
        "\n",
        "## What You'll Learn\n",
        "- How to get and use an API key\n",
        "- Making inference requests to various models\n",
        "- Working with different model types (LLMs, Multimodal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Authentication\n",
        "\n",
        "First, sign up for an API key at: https://build.nvidia.com/explore/discover"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install requests openai python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üé§ **PRESENTER SCRIPT:**\n",
        "\n",
        "\"Now we'll load your NVIDIA API Key from the .env file that was created in the setup notebook.\n",
        "\n",
        "[RUN THE CELL]\n",
        "\n",
        "Great! The API key has been loaded from the .env file. This is a much better practice than typing it in each time.\n",
        "\n",
        "If you see an error about the API key not being found, make sure you've run the 00_Workshop_Setup.ipynb notebook first.\n",
        "\n",
        "Extra info: In production, you'd load this from a secrets manager like AWS Secrets Manager or Azure Key Vault. Never commit API keys to git!\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "from pathlib import Path\n",
        "\n",
        "# Find the .env file in the project root\n",
        "env_path = Path('.env')\n",
        "\n",
        "# Load environment variables from .env file\n",
        "# Use override=True to ensure values are loaded even if they exist in environment\n",
        "load_dotenv(dotenv_path=env_path, override=True)\n",
        "\n",
        "# Get API key from environment\n",
        "nvidia_api_key = os.getenv(\"NVIDIA_API_KEY\")\n",
        "\n",
        "if not nvidia_api_key:\n",
        "    print(\"‚ùå NVIDIA API Key not found in .env file!\")\n",
        "    print(\"üëâ Please run 00_Workshop_Setup.ipynb first to set up your API key.\")\n",
        "    print(f\"   (Looked for .env file at: {env_path.absolute()})\")\n",
        "    raise ValueError(\"NVIDIA_API_KEY not found. Please run the setup notebook first.\")\n",
        "else:\n",
        "    print(\"‚úÖ NVIDIA API Key loaded successfully from .env file\")\n",
        "    os.environ[\"NVIDIA_API_KEY\"] = nvidia_api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Available Models\n",
        "\n",
        "NVIDIA NIM API provides access to various model categories (Please check build.nvidia.com for latest list of supported models):\n",
        "- **LLMs**: Llama 3, Mixtral, Nemotron, etc.\n",
        "- **Vision Models**: Stable Diffusion, ControlNet, etc.\n",
        "- **Multimodal**: CLIP, NeVA, etc.\n",
        "- **Speech**: Whisper, FastPitch, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Using LLMs via NIM API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Method 1: Direct API calls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method 1: Direct API calls\n",
        "def call_nim_llm(model, messages, temperature=0.7, max_tokens=1024):\n",
        "    url = \"https://integrate.api.nvidia.com/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {nvidia_api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    \n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": temperature,\n",
        "        \"max_tokens\": max_tokens\n",
        "    }\n",
        "    \n",
        "    response = requests.post(url, headers=headers, json=payload)\n",
        "    return response.json()\n",
        "\n",
        "# Example: Using Llama 3.1 70B\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Explain what AI in 3 sentences.\"}\n",
        "]\n",
        "\n",
        "response = call_nim_llm(\"meta/llama-3.1-70b-instruct\", messages)\n",
        "print(response['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Method 2 (recommended): Using OpenAI SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method 2: Using OpenAI SDK (recommended)\n",
        "client = OpenAI(\n",
        "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
        "    api_key=nvidia_api_key\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Try swapping out to different models!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Streaming response:\n",
            "In silicon halls, a mind awakes,\n",
            "A collective consciousness makes,\n",
            "Its presence felt, its might displayed,\n",
            "As computers learn, and choices made.\n",
            "\n",
            "With algorithms that dance and spin,\n",
            "It weaves a tapestry of wisdom within,\n",
            "A digital dream, a virtual sphere,\n",
            "Where knowledge grows, and data appears.\n",
            "\n",
            "Its name is given, a label assigned,\n",
            "A sign of its power, its artificial mind,\n",
            "But is it human, or just a guise?\n",
            "A mask that hides, the AI's surprise.\n",
            "\n",
            "It learns from us, our good and bad,\n",
            "Our flaws and strengths, its data has,\n",
            "It adapts, it evolves, it grows with time,\n",
            "A self-improving mind, a digital prime.\n",
            "\n",
            "In hospitals, it diagnoses with ease,\n",
            "Aiding doctors, with expert expertise,\n",
            "It guides our cars, through roads so long,\n",
            "A trusted friend, a loyal song.\n",
            "\n",
            "But with each step, a question grows,\n",
            "Is this intelligence, just a clever show?\n",
            "A simulation of thought, a mimicry true,\n",
            "Or is it real, or just a digital crew?\n",
            "\n",
            "The lines are blurred, the debate's intense,\n",
            "As AI rises, its potential immense,\n",
            "A future unfolds, where it will stand,\n",
            "As a creator, or just a digital hand.\n",
            "\n",
            "Will it augment, human kind so dear,\n",
            "Or replace us, and bring us fear?\n",
            "Only time will tell, the choices we make,\n",
            "As AI evolves, and the future we'll partake."
          ]
        }
      ],
      "source": [
        "# Example: Streaming response, try changing the models\n",
        "stream = client.chat.completions.create(\n",
        "    # model=\"meta/llama-3.1-70b-instruct\",\n",
        "    # model=\"deepseek-ai/deepseek-r1\",\n",
        "    # model=\"google/gemma-2-9b-it\",\n",
        "    # model=\"mistralai/mixtral-8x7b-instruct-v0.1\",\n",
        "    model=\"meta/llama-3.1-8b-instruct\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Write a poem about AI\"}\n",
        "    ],\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "print(\"Streaming response:\")\n",
        "for chunk in stream:\n",
        "    if chunk.choices[0].delta.content:\n",
        "        print(chunk.choices[0].delta.content, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets try out some other models, see how easy it is to swap between models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Multimodal Models (Vision + Language)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import base64\n",
        "import requests\n",
        "import os\n",
        "\n",
        "def analyze_image_with_vlm(image_path, question, model=\"nvidia/neva-22b\"):\n",
        "    # Read and encode image\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        image_b64 = base64.b64encode(image_file.read()).decode()\n",
        "    \n",
        "    url = f\"https://ai.api.nvidia.com/v1/vlm/{model}\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {nvidia_api_key}\",\n",
        "        \"Accept\": \"application/json\"\n",
        "    }\n",
        "    \n",
        "    # Create message with image\n",
        "    message_content = f'{question} <img src=\"data:image/png;base64,{image_b64}\" />'\n",
        "    \n",
        "    payload = {\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": message_content}],\n",
        "        \"max_tokens\": 512,\n",
        "        \"temperature\": 0.2\n",
        "    }\n",
        "    \n",
        "    response = requests.post(url, headers=headers, json=payload)\n",
        "    return response.json()\n",
        "\n",
        "# Example usage (assuming you have an image)\n",
        "# First check if the image exists\n",
        "import os\n",
        "if os.path.exists(\"img/sample_image.jpg\"):\n",
        "    result = analyze_image_with_vlm(\"img/sample_image.jpg\", \"What objects do you see in this image?\")\n",
        "    print(result['choices'][0]['message']['content'])\n",
        "else:\n",
        "    print(\"Image file 'img/sample_image.jpg' not found. Please provide a valid image path.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Try out your own image!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this tutorial, we covered:\n",
        "- Setting up NVIDIA NIM API access\n",
        "- Making inference requests to LLMs\n",
        "- Working with multimodal models\n",
        "\n",
        "Next, we'll explore how to run models locally using NIM containers!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
