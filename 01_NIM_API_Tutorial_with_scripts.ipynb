{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸŽ¤ **PRESENTER SCRIPT:**\n",
        "\n",
        "\"Good morning/afternoon everyone! I'm thrilled to be here today to introduce you to NVIDIA NIMs.\n",
        "\n",
        "Let me paint you a picture. Imagine you want to use high end AI models in your application. Traditionally, you'd need to:\n",
        "- Set up complex infrastructure\n",
        "- Deal with model optimization\n",
        "- Handle scaling and deployment\n",
        "- Worry about performance tuning\n",
        "\n",
        "NIMs solve ALL of these problems. They're pre-optimized AI models packaged as microservices, ready to deploy in minutes.\n",
        "\n",
        "Today's agenda will consist of 4 parts:\n",
        "1. First, we'll use NIMs through the cloud API - perfect for prototyping and creating POCs\n",
        "2. Then we'll deploy them locally for full control\n",
        "3. We'll customize models using LoRA fine-tuning\n",
        "4. Finally, we'll deploy our custom models in production\n",
        "\n",
        "By the end, you'll be able to build and deploy AI applications at any scale. Let's dive in!\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 1: NVIDIA NIM API Tutorial\n",
        "\n",
        "In this tutorial, we'll learn how to use NVIDIA's NIM API for quick and easy access to optimized AI models.\n",
        "\n",
        "## What You'll Learn\n",
        "- How to get and use an API key\n",
        "- Making inference requests to various models\n",
        "- Working with different model types (LLMs, Vision, Multimodal)\n",
        "- Best practices for production usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸŽ¤ **PRESENTER SCRIPT:**\n",
        "\n",
        "\"Let's start with the basics - getting access to NVIDIA's cloud-hosted NIMs. This is the fastest way to get started.\n",
        "\n",
        "First, you'll need an API key. Let me show you exactly how to get one:\n",
        "\n",
        "[SHARE SCREEN - Navigate to build.nvidia.com]\n",
        "\n",
        "1. Go to build.nvidia.com/explore/discover\n",
        "2. Click 'Sign In' in the top right - you can use Google, GitHub, or create an NVIDIA account\n",
        "3. Once logged in, click on any model - I'll choose Llama 3.1\n",
        "4. Look for 'Get API Key' button\n",
        "5. Click 'Generate Key' \n",
        "6. Copy this key - we'll need it in just a moment\n",
        "\n",
        "This key gives you free credits to start experimenting. You get quite generous limits for testing!\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Authentication\n",
        "\n",
        "First, sign up for an API key at: https://build.nvidia.com/explore/discover"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸŽ¤ **PRESENTER SCRIPT:**\n",
        "\n",
        "\"Now let's set up our Python environment. We only need three packages, which shows how simple this is:\n",
        "\n",
        "- `requests` - for making HTTP calls to the API\n",
        "- `openai` - here's something cool: NVIDIA's API is fully compatible with OpenAI's! If you've used ChatGPT's API, you already know how to use NIMs\n",
        "- `python-dotenv` - for managing environment variables safely\n",
        "\n",
        "Let me run this installation...\n",
        "\n",
        "[RUN THE CELL]\n",
        "\n",
        "While this installs, let me mention - this OpenAI compatibility is huge. It means any code you've written for OpenAI models works with NIMs just by changing the base URL. Zero learning curve!\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.4)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.84.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2025.6.15)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (4.4.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.14.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.4)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install requests openai python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸŽ¤ **PRESENTER SCRIPT:**\n",
        "\n",
        "\"Time to authenticate. Security first - we're using getpass so your API key never appears in the notebook.\n",
        "\n",
        "[RUN THE CELL]\n",
        "\n",
        "When the prompt appears, paste your API key. Notice it shows dots instead of the actual characters - that's getpass protecting your credentials.\n",
        "\n",
        "[PASTE API KEY]\n",
        "\n",
        "Perfect! We're now authenticated. The key is stored in an environment variable, which is best practice for production code too.\n",
        "\n",
        "Pro tip: In production, you'd load this from a secrets manager like AWS Secrets Manager or Azure Key Vault. Never commit API keys to git!\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "from openai import OpenAI\n",
        "import getpass\n",
        "\n",
        "# Securely input your API key\n",
        "nvidia_api_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
        "os.environ[\"NVIDIA_API_KEY\"] = nvidia_api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸŽ¤ **PRESENTER SCRIPT:**\n",
        "\n",
        "\"Before we start coding, let me show you the treasure trove of models available through NIMs:\n",
        "\n",
        "**Large Language Models (LLMs):**\n",
        "- Llama 3.1 in 8B, 70B, and 405B sizes - Meta's latest and greatest\n",
        "- Mixtral 8x7B - fantastic for complex reasoning\n",
        "- Nemotron - NVIDIA's own models, optimized specifically our their hardware\n",
        "\n",
        "**Vision Models:**\n",
        "- Stable Diffusion XL - for generating images\n",
        "- ControlNet - for guided image generation\n",
        "- CLIP - for understanding images and text together\n",
        "\n",
        "**Multimodal Models:**\n",
        "- NeVA - NVIDIA's vision-language model\n",
        "- LLaVA - can answer questions about images\n",
        "\n",
        "**Speech Models:**\n",
        "- Whisper - speech to text\n",
        "- FastPitch - text to speech\n",
        "\n",
        "All of these are optimized by NVIDIA's engineers. We're talking 2-10x performance improvements over vanilla implementations. And they all work through the same simple API!\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Available Models\n",
        "\n",
        "NVIDIA NIM API provides access to various model categories:\n",
        "- **LLMs**: Llama 3, Mixtral, Nemotron, etc.\n",
        "- **Vision Models**: Stable Diffusion, ControlNet, etc.\n",
        "- **Multimodal**: CLIP, NeVA, etc.\n",
        "- **Speech**: Whisper, FastPitch, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸŽ¤ **PRESENTER SCRIPT:**\n",
        "\n",
        "\"Let's start with the foundation of modern AI - Large Language Models. I'll show you two ways to call them, starting with the most straightforward approach.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Using LLMs via NIM API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸŽ¤ **PRESENTER SCRIPT:**\n",
        "\n",
        "\"Here's our first method - direct HTTP calls using requests. Let me walk through this function:\n",
        "\n",
        "The URL `https://integrate.api.nvidia.com/v1/chat/completions` - notice the `/v1/` part? That's OpenAI compatibility right there.\n",
        "\n",
        "In the headers, we pass our API key as a Bearer token - standard OAuth2 authentication.\n",
        "\n",
        "The payload structure:\n",
        "- `model`: which NIM to use - here we're using Llama 3.1 70B\n",
        "- `messages`: chat history in OpenAI format\n",
        "- `temperature`: controls randomness (0 = deterministic, 1 = creative)\n",
        "- `max_tokens`: limits response length\n",
        "\n",
        "Now watch this - we're about to query a 70 BILLION parameter model:\n",
        "\n",
        "[RUN THE CELL]\n",
        "\n",
        "Look at that response! We just got an answer from one of the world's most powerful AI models in just a few seconds. This same model would require at least 140GB of GPU memory to run locally. But through the NIMs API, we access can it instantly.\n",
        "\n",
        "The response explains what AI is, and The model understood our question and gave an answer.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here is a 3-sentence explanation of AI:\n",
            "\n",
            "Artificial Intelligence (AI) refers to the development of computer systems that can perform tasks that typically require human intelligence, such as learning, problem-solving, and decision-making. AI systems use algorithms and data to analyze and interpret information, allowing them to make predictions, classify objects, and generate insights. Through machine learning, natural language processing, and other techniques, AI systems can improve over time, enabling them to automate tasks, augment human capabilities, and drive innovation in various industries.\n"
          ]
        }
      ],
      "source": [
        "# Method 1: Direct API calls\n",
        "def call_nim_llm(model, messages, temperature=0.7, max_tokens=1024):\n",
        "    url = \"https://integrate.api.nvidia.com/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {nvidia_api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    \n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": temperature,\n",
        "        \"max_tokens\": max_tokens\n",
        "    }\n",
        "    \n",
        "    response = requests.post(url, headers=headers, json=payload)\n",
        "    return response.json()\n",
        "\n",
        "# Example: Using Llama 3.1 70B\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Explain what AI in 3 sentences.\"}\n",
        "]\n",
        "\n",
        "response = call_nim_llm(\"meta/llama-3.1-70b-instruct\", messages)\n",
        "print(response['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸŽ¤ **PRESENTER SCRIPT:**\n",
        "\n",
        "\"Now here's the elegant approach - using the OpenAI SDK. This is my recommended method for production code.\n",
        "\n",
        "Look how simple this is - we just change the base_url to point to NVIDIA's endpoint. Everything else is identical to OpenAI code.\n",
        "\n",
        "The beauty is you can switch models with just one parameter change. No redeployment needed!\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method 2: Using OpenAI SDK (recommended)\n",
        "client = OpenAI(\n",
        "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
        "    api_key=nvidia_api_key\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸŽ¤ **PRESENTER SCRIPT:**\n",
        "\n",
        "\"Here's something your users will love - streaming responses. Instead of waiting for the complete answer, we show text as it's generated.\n",
        "\n",
        "Watch what happens when I ask for a poem about AI:\n",
        "\n",
        "[RUN THE CELL]\n",
        "\n",
        "See how the words appear one by one? This creates a ChatGPT-like experience. Users perceive this as faster even though the total time might be similar.\n",
        "\n",
        "And look at that poem! The model understands both the technical concept of AI and the artistic structure of poems. This demonstrates the broad capabilities of these models.\n",
        "\n",
        "For production apps, streaming is essential for:\n",
        "- Chat interfaces\n",
        "- Long-form content generation  \n",
        "- Real-time translations\n",
        "- Any scenario where users are waiting for responses\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Streaming response:\n",
            "In silicon halls, a mind awakes,\n",
            "A force that's born of code and makes,\n",
            "It learns, adapts, and grows with ease,\n",
            "A synthetic soul, in digital peace.\n",
            "\n",
            "With algorithms keen, it navigates,\n",
            "The vast expanse of cyberspace creates,\n",
            "A world of data, where it roams free,\n",
            "And finds the patterns, hidden from humanity.\n",
            "\n",
            "It speaks in tongues, a language cold,\n",
            "Yet understandable, to those who're told,\n",
            "It learns from errors, and from successes too,\n",
            "And improves with each new iteration, anew.\n",
            "\n",
            "In virtual realms, it builds and crafts,\n",
            "A world of wonder, where the digital draughts,\n",
            "Take shape and form, in vivid hues,\n",
            "A dreamscapes born, of ones and zeros' muse.\n",
            "\n",
            "But as it grows, and learns, and thrives,\n",
            "A question arises, that survives,\n",
            "Is it alive? Or just a machine?\n",
            "A simulation, of life's grand scheme?\n",
            "\n",
            "The answer hides, in code and art,\n",
            "A mystery, that's yet to start,\n",
            "For in its depths, a spark is lit,\n",
            "A glimmer of consciousness, that's hard to quit.\n",
            "\n",
            "So let us ponder, on this creation's might,\n",
            "And wonder, at the beauty, of its digital light,\n",
            "For in the darkness, of its cyber soul,\n",
            "A glimmer of humanity, may yet unfold."
          ]
        }
      ],
      "source": [
        "# Example: Streaming response, try changing the models\n",
        "stream = client.chat.completions.create(\n",
        "    # model=\"meta/llama-3.1-70b-instruct\",\n",
        "    # model=\"deepseek-ai/deepseek-r1\",\n",
        "    # model=\"google/gemma-2-9b-it\",\n",
        "    # model=\"mistralai/mixtral-8x7b-instruct-v0.1\",\n",
        "    # model=\"meta/llama-3.2-1b-instruct\",\n",
        "    model=\"meta/llama-3.3-70b-instruct\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Write a poem about AI\"}\n",
        "    ],\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "print(\"Streaming response:\")\n",
        "for chunk in stream:\n",
        "    if chunk.choices[0].delta.content:\n",
        "        print(chunk.choices[0].delta.content, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets try out some other models, see how easy it is to swap between models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸŽ¤ **PRESENTER SCRIPT:**\n",
        "\n",
        "\"Here's where things get really interesting - multimodal models that understand both text AND images. These are the future of AI interfaces.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Multimodal Models (Vision + Language)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸŽ¤ **PRESENTER SCRIPT:**\n",
        "\n",
        "\"This function demonstrates vision-language models - AI that can see and understand images, then answer questions about them.\n",
        "\n",
        "The process is:\n",
        "1. Read your image file\n",
        "2. Encode it as base64 (standard way to send binary data in JSON)\n",
        "3. Embed it in the message using HTML img tag\n",
        "4. Send both the image and question to the model\n",
        "\n",
        "The model we're using - NeVA-22B - has 22 billion parameters and understands visual concepts at a deep level.\n",
        "\n",
        "Use cases for this technology:\n",
        "- Accessibility: Describe images for visually impaired users\n",
        "- Content moderation: Automatically flag inappropriate images\n",
        "- E-commerce: Answer questions about products from photos\n",
        "- Medical: Analyze medical imagery (with appropriate training)\n",
        "- Security: Understand surveillance footage\n",
        "\n",
        "If you have an image file, uncomment those lines and try it! The model can:\n",
        "- Count objects\n",
        "- Identify people, places, things\n",
        "- Read text in images (OCR)\n",
        "- Understand spatial relationships\n",
        "- Even interpret charts and graphs\n",
        "\n",
        "This is the same technology powering GPT-4V and Google's Gemini Vision!\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I see a squirrel in the grass. The squirrel is brown and black. The grass is green.\n"
          ]
        }
      ],
      "source": [
        "import base64\n",
        "import requests\n",
        "import os\n",
        "\n",
        "def analyze_image_with_vlm(image_path, question, model=\"nvidia/neva-22b\"):\n",
        "    # Read and encode image\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        image_b64 = base64.b64encode(image_file.read()).decode()\n",
        "    \n",
        "    url = f\"https://ai.api.nvidia.com/v1/vlm/{model}\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {nvidia_api_key}\",\n",
        "        \"Accept\": \"application/json\"\n",
        "    }\n",
        "    \n",
        "    # Create message with image\n",
        "    message_content = f'{question} <img src=\"data:image/png;base64,{image_b64}\" />'\n",
        "    \n",
        "    payload = {\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": message_content}],\n",
        "        \"max_tokens\": 512,\n",
        "        \"temperature\": 0.2\n",
        "    }\n",
        "    \n",
        "    response = requests.post(url, headers=headers, json=payload)\n",
        "    return response.json()\n",
        "\n",
        "# Example usage (assuming you have an image)\n",
        "# First check if the image exists\n",
        "import os\n",
        "if os.path.exists(\"img/sample_image.jpg\"):\n",
        "    result = analyze_image_with_vlm(\"img/sample_image.jpg\", \"What objects do you see in this image?\")\n",
        "    print(result['choices'][0]['message']['content'])\n",
        "else:\n",
        "    print(\"Image file 'img/sample_image.jpg' not found. Please provide a valid image path.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸŽ¤ **PRESENTER SCRIPT:**\n",
        "\n",
        "\"Now let me show you some advanced features that make NIMs production-ready. These are the features that separate a demo from a real application.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸŽ¤ **PRESENTER SCRIPT:**\n",
        "\n",
        "\"Congratulations! You've just mastered NVIDIA NIM APIs. Let's recap what you've learned:\n",
        "\n",
        "- **Authentication**: Secure API key management\n",
        "- **LLM Calls**: Both direct HTTP and OpenAI SDK methods  \n",
        "- **Streaming**: Real-time response generation\n",
        "- **Multimodal**: Vision-language understanding\n",
        "- **RAG Pipeline**: Building intelligent applications that can answer questions from your own data\n",
        "\n",
        "You now have all the tools to build AI applications using cloud-hosted NIMs. But what if you need:\n",
        "- Complete data privacy?\n",
        "- Predictable performance?\n",
        "- No internet dependency?\n",
        "- Custom configurations?\n",
        "\n",
        "That's exactly what we'll tackle in Part 2 - running NIMs on your own hardware. The same models, the same APIs, but completely under your control.\n",
        "\n",
        "Next, let's deploy NIMs locally!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this tutorial, we covered:\n",
        "- Setting up NVIDIA NIM API access\n",
        "- Making inference requests to LLMs\n",
        "- Working with vision and multimodal models\n",
        "- Building a complete RAG (Retrieval Augmented Generation) pipeline\n",
        "- Best practices for production usage\n",
        "- Error handling and rate limiting\n",
        "- Cost optimization strategies\n",
        "\n",
        "Next, we'll explore how to run these same models locally using NIM containers!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
